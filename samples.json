[{"prefix": "from test_env.helpers import (\n    handle_child_death,\n    AlertingServiceHandle,\n    MockServiceHandle,\n    PingingJob,\n    MailServer,\n    scrap_ack_url,\n    clear_db\n)\nimport signal\nfrom time import sleep\n\n\nLOGS_DIR = '../../logs'\n\n\ndef test_sending_alert():\n    orig = signal.signal(signal.SIGCHLD, handle_child_death)\n    mail_server = MailServer(port=1025)\n    sleep(2)\n    alert_service = AlertingServiceHandle(LOGS_DIR)\n    mock_service = MockServiceHandle(7000, LOGS_DIR)\n    try:\n        sleep(5)\n        new_job = PingingJob(\"dziekan@localhost\", \"student@localhost\", 100, mock_service, 1000, 1000)\n        alert_service.add_pinging_job(new_job)\n        mock_service.respond_404()\n        sleep(3)\n        assert mail_server.last_mail_to(\"dziekan@localhost\") is not None\n        sleep(1)\n        assert mail_server.last_mail_to(\"student@localhost\") is not None\n\n        mock_service.respond_normal()\n        new_job = PingingJob(\"piwo@localhost\", \"sesja@localhost\", 100, mock_service, 1000, 5000)\n        alert_service.add_pinging_job(new_job)\n        mock_service.respond_timeout()\n        sleep(3)\n        mail_content = mail_server.last_mail_to(\"piwo@localhost\")\n        assert mail_content is not None\n        ack_link = scrap_ack_url(mail_content)\n        alert_service.confirm_alert(ack_link)\n        sleep(5)\n        assert mail_server.last_mail_to(\"sesja@localhost\") is None\n    finally:\n        signal.signal(signal.SIGCHLD, orig)\n        mail_server.stop()\n        mock_service.close()\n        alert_service.close()\n        clear_db()\n\n\ndef test_normal_behavior():\n    orig = signal.signal(signal.SIGCHLD, handle_child_death)\n    mail_server = MailServer(port=1025)\n    sleep(2)\n    alert_service = AlertingServiceHandle(LOGS_DIR)\n    mock_service = MockServiceHandle(7000, LOGS_DIR)\n    sleep(5)\n    try:\n        new_job = PingingJob(\"poker@localhost\", \"calka@localhost\", 100, mock_service, 1000, 5000)\n        alert_service.add_pinging_job(new_job)\n        sleep(10)\n        assert mail_server.last_mail_to(\"poker@localhost\") is None\n        assert mail_server.last_mail_to(\"calka@localhost\") is None\n    finally:\n        signal.signal(signal.SIGCHLD, orig)\n        mail_server.stop()\n        mock_service.close()\n        alert_service.close()\n        clear_db()\n\n\ndef test_deleting_job():\n    orig = signal.signal(signal.SIGCHLD, handle_child_death)\n    mail_server = MailServer(port=1025)\n    sleep(2)\n    alert_service = AlertingServiceHandle(LOGS_DIR)\n    mock_service = MockServiceHandle(7000, LOGS_DIR)\n    sleep(5)\n    try:\n        job = PingingJob(\"mail1@localhost\", \"mail2@localhost\", 100, mock_service, 1000, 5000)\n        assert mock_service.get_pings_received() == 0\n        job_id = alert_service.add_pinging_job(job)\n        sleep(1)\n        assert mock_service.get_pings_received() > 0\n        alert_service.remove_pinging_job(job_id)\n        sleep(3) # after that number of pings should stabilize\n        pings_received = mock_service.get_pings_received()\n        sleep(1)\n        assert (t:=mock_service.get_pings_received()) == pings_received, f\"{t}!={pings_received}\"\n    finally:\n        signal.signal(signal.SIGCHLD, orig)\n        mail_server.stop()\n        mock_service.close()\n        alert_service.close()\n        clear_db()\n\n\ndef test_recovery():\n    orig = signal.signal(signal.SIGCHLD, handle_child_death)\n    mail_server = MailServer(port=1025)\n    sleep(2)\n    alert_service = AlertingServiceHandle(LOGS_DIR)\n    mock_service = MockServiceHandle(7000, LOGS_DIR)\n  ", "suffix": "        sleep(5)\n        new_job = PingingJob(\"dziekan@localhost\", \"student@localhost\", 100, mock_service, 1000, 1000)\n        alert_service.add_pinging_job(new_job)\n\n        signal.signal(signal.SIGCHLD, lambda signum, frame: None)\n        alert_service.close()\n        sleep(1)\n        signal.signal(signal.SIGCHLD, handle_child_death)\n        alert_service = AlertingServiceHandle(LOGS_DIR)\n\n        mock_service.respond_404()\n        sleep(3)\n        assert mail_server.last_mail_to(\"dziekan@localhost\") is not None\n        sleep(1)\n        assert mail_server.last_mail_to(\"student@localhost\") is not None\n\n    finally:\n        signal.signal(signal.SIGCHLD, orig)\n        mail_server.stop()\n        mock_service.close()\n        alert_service.close()\n        clear_db()\n\n\nif __name__ == '__main__':\n\n    test_sending_alert()\n    sleep(0.5)\n    test_normal_behavior()\n    sleep(0.5)\n    test_deleting_job()\n    sleep(0.5)\n    test_recovery()\n    exit(0)\n", "missing": "  try:\n"}, {"prefix": "from test_env.helpers import (\n    handle_child_death,\n    AlertingServiceHandle,\n    MockServiceHandle,\n    PingingJob,\n    MailServer,\n    scrap_ack_url,\n    clear_db\n)\nimport signal\nfrom time import sleep\n\n\nLOGS_DIR = '../../logs'\n\n\ndef test_sending_alert():\n    orig = signal.signal(signal.SIGCHLD, handle_child_death)\n    mail_server = MailServer(port=1025)\n    sleep(2)\n    alert_service = AlertingServiceHandle(LOGS_DIR)\n    mock_service = MockServiceHandle(7000, LOGS_DIR)\n    try:\n        sleep(5)\n        new_job = PingingJob(\"dziekan@localhost\", \"student@localhost\", 100, mock_service, 1000, 1000)\n        alert_service.add_pinging_job(new_job)\n        mock_service.respond_404()\n        sleep(3)\n        assert mail_server.last_mail_to(\"dziekan@localhost\") is not None\n        sleep(1)\n        assert mail_server.last_mail_to(\"student@localhost\") is not None\n\n        mock_service.respond_normal()\n        new_job = PingingJob(\"piwo@localhost\", \"sesja@localhost\", 100, mock_service, 1000, 5000)\n        alert_service.add_pinging_job(new_job)\n        mock_service.respond_timeout()\n        sleep(3)\n        mail_content = mail_server.last_mail_to(\"piwo@localhost\")\n        assert mail_content is not None\n        ack_link = scrap_ack_url(mail_content)\n        alert_service.confirm_alert(ack_link)\n        sleep(5)\n        assert mail_server.last_mail_to(\"sesja@localhost\") is None\n    finally:\n        signal.signal(signal.SIGCHLD, orig)\n        mail_server.stop()\n        mock_service.close()\n        alert_service.close()\n        clear_db()\n\n\ndef test_normal_behavior():\n    orig = signal.signal(signal.SIGCHLD, handle_child_death)\n    mail_server = MailServer(port=1025)\n    sleep(2)\n    alert_service = AlertingServiceHandle(LOGS_DIR)\n    mock_service = MockServiceHandle(7000, LOGS_DIR)\n    sleep(5)\n    try:\n        new_job = PingingJob(\"poker@localhost\", \"calka@localhost\", 100, mock_service, 1000, 5000)\n        alert_service.add_pinging_job(new_job)\n        sleep(10)\n        assert mail_server.last_mail_to(\"poker@localhost\") is None\n        assert mail_server.last_mail_to(\"calka@localhost\") is None\n    finally:\n        signal.signal(signal.SIGCHLD, orig)\n        mail_server.stop()\n        mock_service.close()\n        alert_service.close()\n        clear_db()\n\n\ndef test_deleting_job():\n    orig = signal.signal(signal.SIGCHLD, handle_child_death)\n    mail_server = MailServer(port=1025)\n    sleep(2)\n    alert_service = AlertingServiceHandle(LOGS_DIR)\n    mock_service = MockServiceHandle(7000, LOGS_DIR)\n    sleep(5)\n    try:\n        job = PingingJob(\"mail1@localhost\", \"mail2@localhost\", 100, mock_service, 1000, 5000)\n        assert mock_service.get_pings_received() == 0\n        job_id = alert_service.add_pinging_job(job)\n        sleep(1)\n        assert mock_service.get_pings_received() > 0\n        alert_service.remove_pinging_job(job_id)\n        sleep(3) # after that number of pings should stabilize\n        pings_received = mock_service.get_pings_received()\n        sleep(1)\n        assert (t:=mock_service.get_pings_received()) == pings_received, f\"{t}!={pings_received}\"\n    finally:\n        signal.signal(signal.SIGCHLD, orig)\n        mail_server.stop()\n        mock_service.close()\n        alert_service.close()\n        clear_db()\n\n\ndef test_recovery():\n    orig = signal.signal(signal.SIGCHLD, handle_child_death)\n    mail_server = MailServer(port=1025)\n    sleep(2)\n    alert_service = AlertingServiceHandle(LOGS_DIR)\n    mock_service = MockServiceHandle(7000, LOGS_DIR)\n    try:\n        sleep(5)\n        new_job = PingingJob(\"dziekan@localhost\", \"student@localhost\", 100, mock_service, 1000, 1000)\n        alert_service.add_pinging_job(new_job)\n\n        signal.signal(signal.SIGCHLD, lambda signum, frame: None)\n        alert_service.close()\n  ", "suffix": "        signal.signal(signal.SIGCHLD, handle_child_death)\n        alert_service = AlertingServiceHandle(LOGS_DIR)\n\n        mock_service.respond_404()\n        sleep(3)\n        assert mail_server.last_mail_to(\"dziekan@localhost\") is not None\n        sleep(1)\n        assert mail_server.last_mail_to(\"student@localhost\") is not None\n\n    finally:\n        signal.signal(signal.SIGCHLD, orig)\n        mail_server.stop()\n        mock_service.close()\n        alert_service.close()\n        clear_db()\n\n\nif __name__ == '__main__':\n\n    test_sending_alert()\n    sleep(0.5)\n    test_normal_behavior()\n    sleep(0.5)\n    test_deleting_job()\n    sleep(0.5)\n    test_recovery()\n    exit(0)\n", "missing": "      sleep(1)\n"}, {"prefix": "from datetime import datetime\nfrom sys import stdout\nfrom typing import TextIO\nfrom os import write\n\nlogging_target = stdout\n\n\ndef set_logging_target(file: TextIO):\n    global logging_target\n    logging_target = file\n\n\ndef close_logging_target():\n    global logging_target\n    logging_target.close()\n\n\ndef log(message, lvl):\n    write(logging_target.fileno(), bytes(f\"[{lvl}] {datetime.now()}: {message}\\n\", encoding=\"utf-8\"))\n\n\ndef info(message):\n    log(message, \"INFO\")\n\n\ndef warn(message):\n    log(message, \"WARN\")\n\n\ndef error(message):\n    log(message, \"ERROR\")\n\n\ndef debug(message):\n    log(", "suffix": "\n\ndef log_net(lvl_callable, message, service, port):\n    lvl_callable(f\"communication with {service} at {port}: {message}\")\n", "missing": "message, \"DEBUG\")\n"}, {"prefix": "from datetime import datetime\nfrom sys import stdout\nfrom typing import TextIO\nfrom os import write\n\nlogging_target = stdout\n\n\ndef set_logging_target(file: TextIO):\n    global logging_target\n    logging_target = file\n\n\ndef close_logging_target():\n    global logging_target\n    logging_target.close()\n\n\ndef log(message, lvl):\n    write(logging_target.fileno(), bytes(f\"[{lvl}] {datetime.now()}: {message}\\n\", encoding=\"utf-8\"))\n\n\ndef info(message):\n    log(message, \"INFO\")\n\n\ndef warn(message):\n    log(message, \"WARN\")\n\n\ndef error(message):\n    log(message, \"ERROR\")\n\n\ndef debug(message):\n    log(message, \"DEBUG\")\n\n\ndef log_net(lvl_callable, message, service, port):\n    lvl_callable(f\"com", "suffix": "", "missing": "munication with {service} at {port}: {message}\")\n"}, {"prefix": "import signal\nimport sys\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nimport os\nimport subprocess\nimport requests\nfrom sys import exit\nfrom test_env.log import *\n\nimport threading\nfrom aiosmtpd.controller import Controller\nfrom aiosmtpd.handlers import Debugging\nfrom email.message import EmailMessage\nfrom email import message_from_bytes, policy\nimport psycopg2\n\nfrom test_env.log import log_net\n\n\nclass SMTPHandler(Debugging):\n    def __init__(self, collector, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.collector = collector\n\n\n    async def handle_DATA(self, server, session, envelope):\n        try:\n            # Convert raw email bytes to an EmailMessage object\n            email_message = message_from_bytes(envelope.content)\n\n            self.collector(email_message)\n\n            info(f\"Email received for account: {email_message['To']} from {envelope.mail_from}\")\n            return '250 OK'\n        except Exception as e:\n            warn(f\"Error handling email: {e}\")\n            return '451 Internal Server Error'\n\n\ndef save_mail(msg: EmailMessage):\n    with open(\"mail.log\", \"a\") as mail_log:\n        content = msg.as_string().replace('\\n', ' ').replace('\\r', ' ')\n        mail_log.write(f\"{msg['To']};{content}\\n\")\n\n\ndef scrap_ack_url(mail_content: str) -> str:\n    try:\n        ix1 = mail_content.index(\"Click\") + len(\"Click\")\n        mail_content = mail_content[ix1:]\n        ix2 = mail_content.index(\" to acknowledge\")\n        mail_content = mail_content[:ix2]\n        return mail_content\n    except Exception as e:\n        error(f\"notification id could not be scrapped from email {e}\")\n        return \"\"\n\n\nDB_USER = os.getenv(\"DB_USER\")\nDB_PASSWORD = os.getenv(\"DB_PASS\")\nDB_NAME = os.getenv(\"DB_NAME\")\nDB_HOST = \"localhost\"\nDB_PORT = 5432\n\n\ndef clear_db():\n    try:\n        conn = psycopg2.connect(\n            host=DB_HOST,\n            port=DB_PORT,\n            user=DB_USER,\n            password=DB_PASSWORD,\n            database=DB_NAME,\n        )\n\n        cursor = conn.cursor()\n        setup_file = \"../../server/db_migrations/V2__setup.sql\"\n        with open(setup_file, 'r') as file:\n            sql_script = file.read()\n        cursor.execute(sql_script)\n        conn.commit()\n    except Exception as e:\n        error(\"error on clearing database: {}\".format(e))\n        exit(1)\n\n\nclass MailServer:\n    def __init__(self, host=\"localhost\", port=587):\n        self.host = host\n        self.port = port\n        self.controller = Controller(handler=SMTPHandler(save_mail), hostname=self.host, port=self.port)\n        self.thread = None\n        self.start()\n\n\n    def start(self):\n        if self.thread is not None:\n            warn(\"Mail server is already running.\")\n            return\n\n        def run():\n            self.controller.start()\n\n        self.thread = threading.Thread(target=run, daemon=True)\n        self.thread.start()\n        info(f\"Mail server started on {self.host}:{self.port}\")\n\n    def stop(self):\n        if self.thread is None:\n            warn(\"Mail server is not running.\")\n            return\n\n        self.controller.stop()\n        self.thread = None\n        with open(\"mail.log\", \"w\"):\n            pass\n        info(\"Mail server stopped.\")\n\n    def last_mail_to(self, receiver: str) -> Optional[str]:\n        content = None\n        with open(\"mail.log\", \"a+\") as mail_log:\n            mail_log.seek(0)\n            for line in mail_log:\n                if line.strip().split(';')[0] == receiver:\n                    content = line.strip().split(';', 1)[1]\n        return content\n\n\nclass MockServiceHandle:\n    serial_no = 0\n    def __init__(self, port: int, log_dir: str):\n        self.port = port\n        self.log_file_path = f\"{log_dir}/service-{self.port}-{MockServiceHandle.serial_no}.log\"\n        MockServiceHandle.serial_no += 1\n        self.child_process = None\n        self._create()\n        \n    def _create(self):\n        log_net(info, \"Creating...\", \"mock service\", self.port)\n        with open(self.log_file_path, \"a\") as log_file:\n            self.child_process = subprocess.Popen([\"python\", \"test_env/mock_server.py\", \"localhost\", str(self.port)], stdout=log_file, stderr=log_file)\n\n    def close(self):\n        os.kill(self.child_process.pid, signal.SIGINT)\n\n    def _set_mode(self, mode: str):\n        resp = requests.post(f\"http://localhost:{self.port}/set_response_mode?mode={mode}\")\n        success = 200 <= resp.status_code < 300\n        if not success:\n            log_net(warn, f\"failed to set resp mode to {mode}\", \"mock_service\", self.port)\n        else:\n            log_net(info, f\"set resp mode to {mode}\", \"mock_service\", self.port)\n        return success\n\n    def respond_timeout(self):\n        self._set_mode(\"timeout\")\n    \n    def respond_normal(self):\n        self._set_mode(\"normal\")\n    \n    def respond_404(self):\n        self._set_mode(\"404\")\n\n    def get_pings_received(self) -> int:\n        resp = requests.get(f\"http://localhost:{self.port}/get_pings_received\")\n        success = 200 <= resp.status_code < 300\n        if not success:\n            log_net(warn, f\"failed to get number of pings received\", \"mock_service\", self.port)\n        else:\n            log_net(info, f\"got number of pings received\", \"mock_service\", self.port)\n        return int(resp.text)\n\n\n@dataclass\nclass PingingJob:\n    mail1: str\n    mail2: str\n    period: int\n    url: str | MockServiceHandle\n    alerting_window: int\n    response_time: int\n\n\ndef handle_child_death(signum, frame):\n    error(\"Child death detected.\\n Terminating.\")\n    exit(1)\n\n\nclass AlertingServiceHandle:\n    serial_no = 0\n    def __init__(self, log_dir: str):\n        log_net(info, \"Creating...\", \"alerting service\", 8080)\n        self.port = 8080\n        self.log_filename = f\"{log_dir}/alert-{self.port}-{AlertingServiceHandle.serial_no}.log\"\n        AlertingServiceHandle.serial_no += 1\n        log_file = open(self.log_filename, \"a\")\n        self.child_process = None\n        self.child_process = subprocess.Popen([\"python\", \"../../server/main.py\", \">\", \"server.log\"], stdout=log_file, stderr=log_file)\n        log_file.close()\n\n\n    def add_pinging_job(self, job_data: PingingJob) -> int:\n        payload = {\n            \"url\": job_data.url if isinstance(job_data.url, str) else f\"http://localhost:{job_data.url.port}/pinging_endpoint\",\n            \"alerting_window\": job_data.alerting_window,\n            \"period\": job_data.period,\n            \"primary_email\": job_data.mail1,\n            \"secondary_email\": job_data.mail2,\n            \"response_time\": job_data.response_time\n        }\n        resp = requests.post(f\"http://localhost:{self.port}/add_service\", json=payload)\n        try:\n            job_id = resp.json()[\"job_id\"]\n            log_net(info, f\"added ping job {job_id}\", self.__class__.__name__, self.port)\n            return job_id\n        except (requests.exceptions.JSONDecodeError, KeyError) as e:\n            log_net(warn, f\"failed to add ping job. Exception: {type(e).__name__}, reason: {e}, server_resp: {resp} {resp.json()}\", self.__class__.__name__, self.port)\n            return -1\n\n\n    def remove_pinging_job(\n            self, job_id: int\n    ) -> bool:\n        resp = requests.delete(f\"http://localhost:{self.port}/del_job?job_id={job_id}\")\n        success = 200 <= resp.status_code < 300\n        if not success:\n            log_net(warn, f\"failed to delete ping job with id {job_id}. response code: {resp.status_code}\", self.__class__.__name__, self.port)\n        else:\n            log_net(info, f\"deleted ping job with id {job_id}\", self.__class__.__name__, self.port)\n        return success\n\n\n    def get_pinging_jobs(\n            self, mail: str,\n    ) -> Optional[List[PingingJob]]:\n        payload = {\n            \"primary_admin_email\": mail,\n        }\n        resp = requests.get(f\"http://localhost:{self.port}/alerting_jobs\", payload)\n        if resp.status_code != 200:\n            log_net(warn, f\"failed to get pinging for mail {mail}. response code: {resp.status_code}\", self.__class__.__name__, self.port)\n            return None\n        res = []\n        try:\n            for entry in resp.json():\n                res.append(PingingJob(**entry))\n        except (requests.exceptions.JSONDecodeError, Exception) as e:\n            log_net(warn, f\"failed to get pinging jobs. Exception: {type(e).__name__}, reason: {e}\", self.__class__.__name__, self.port)\n            return None\n        log_net(info, f\"get pinging jobs for mail {mail}\", self.__class__.__name__, self.port)\n        return res\n\n\n    def ", "suffix": "\n        resp = requests.get(link)\n\n        if 200 <= resp.status_code < 300:\n            log_net(info, f\"acknowledged alert: {link}\", self.__class__.__name__, self.port)\n            return True\n        else:\n            log_net(warn, f\"failed to acknowledge alert with link {link}. response code: {resp.status_code}, response: {resp.text}\", self.__class__.__name__, self.port)\n            return False\n\n\n    def close(self):\n        os.kill(self.child_process.pid, signal.SIGTERM)\n", "missing": "confirm_alert(self, link: str) -> bool:\n"}, {"prefix": "import signal\nimport sys\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nimport os\nimport subprocess\nimport requests\nfrom sys import exit\nfrom test_env.log import *\n\nimport threading\nfrom aiosmtpd.controller import Controller\nfrom aiosmtpd.handlers import Debugging\nfrom email.message import EmailMessage\nfrom email import message_from_bytes, policy\nimport psycopg2\n\nfrom test_env.log import log_net\n\n\nclass SMTPHandler(Debugging):\n    def __init__(self, collector, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.collector = collector\n\n\n    async def handle_DATA(self, server, session, envelope):\n        try:\n            # Convert raw email bytes to an EmailMessage object\n            email_message = message_from_bytes(envelope.content)\n\n            self.collector(email_message)\n\n            info(f\"Email received for account: {email_message['To']} from {envelope.mail_from}\")\n            return '250 OK'\n        except Exception as e:\n            warn(f\"Error handling email: {e}\")\n            return '451 Internal Server Error'\n\n\ndef save_mail(msg: EmailMessage):\n    with open(\"mail.log\", \"a\") as mail_log:\n        content = msg.as_string().replace('\\n', ' ').replace('\\r', ' ')\n        mail_log.write(f\"{msg['To']};{content}\\n\")\n\n\ndef scrap_ack_url(mail_content: str) -> str:\n    try:\n        ix1 = mail_content.index(\"Click\") + len(\"Click\")\n        mail_content = mail_content[ix1:]\n        ix2 = mail_content.index(\" to acknowledge\")\n        mail_content = mail_content[:ix2]\n        return mail_content\n    except Exception as e:\n        error(f\"notification id could not be scrapped from email {e}\")\n        return \"\"\n\n\nDB_USER = os.getenv(\"DB_USER\")\nDB_PASSWORD = os.getenv(\"DB_PASS\")\nDB_NAME = os.getenv(\"DB_NAME\")\nDB_HOST = \"localhost\"\nDB_PORT = 5432\n\n\ndef clear_db():\n    try:\n        conn = psycopg2.connect(\n   ", "suffix": "            port=DB_PORT,\n            user=DB_USER,\n            password=DB_PASSWORD,\n            database=DB_NAME,\n        )\n\n        cursor = conn.cursor()\n        setup_file = \"../../server/db_migrations/V2__setup.sql\"\n        with open(setup_file, 'r') as file:\n            sql_script = file.read()\n        cursor.execute(sql_script)\n        conn.commit()\n    except Exception as e:\n        error(\"error on clearing database: {}\".format(e))\n        exit(1)\n\n\nclass MailServer:\n    def __init__(self, host=\"localhost\", port=587):\n        self.host = host\n        self.port = port\n        self.controller = Controller(handler=SMTPHandler(save_mail), hostname=self.host, port=self.port)\n        self.thread = None\n        self.start()\n\n\n    def start(self):\n        if self.thread is not None:\n            warn(\"Mail server is already running.\")\n            return\n\n        def run():\n            self.controller.start()\n\n        self.thread = threading.Thread(target=run, daemon=True)\n        self.thread.start()\n        info(f\"Mail server started on {self.host}:{self.port}\")\n\n    def stop(self):\n        if self.thread is None:\n            warn(\"Mail server is not running.\")\n            return\n\n        self.controller.stop()\n        self.thread = None\n        with open(\"mail.log\", \"w\"):\n            pass\n        info(\"Mail server stopped.\")\n\n    def last_mail_to(self, receiver: str) -> Optional[str]:\n        content = None\n        with open(\"mail.log\", \"a+\") as mail_log:\n            mail_log.seek(0)\n            for line in mail_log:\n                if line.strip().split(';')[0] == receiver:\n                    content = line.strip().split(';', 1)[1]\n        return content\n\n\nclass MockServiceHandle:\n    serial_no = 0\n    def __init__(self, port: int, log_dir: str):\n        self.port = port\n        self.log_file_path = f\"{log_dir}/service-{self.port}-{MockServiceHandle.serial_no}.log\"\n        MockServiceHandle.serial_no += 1\n        self.child_process = None\n        self._create()\n        \n    def _create(self):\n        log_net(info, \"Creating...\", \"mock service\", self.port)\n        with open(self.log_file_path, \"a\") as log_file:\n            self.child_process = subprocess.Popen([\"python\", \"test_env/mock_server.py\", \"localhost\", str(self.port)], stdout=log_file, stderr=log_file)\n\n    def close(self):\n        os.kill(self.child_process.pid, signal.SIGINT)\n\n    def _set_mode(self, mode: str):\n        resp = requests.post(f\"http://localhost:{self.port}/set_response_mode?mode={mode}\")\n        success = 200 <= resp.status_code < 300\n        if not success:\n            log_net(warn, f\"failed to set resp mode to {mode}\", \"mock_service\", self.port)\n        else:\n            log_net(info, f\"set resp mode to {mode}\", \"mock_service\", self.port)\n        return success\n\n    def respond_timeout(self):\n        self._set_mode(\"timeout\")\n    \n    def respond_normal(self):\n        self._set_mode(\"normal\")\n    \n    def respond_404(self):\n        self._set_mode(\"404\")\n\n    def get_pings_received(self) -> int:\n        resp = requests.get(f\"http://localhost:{self.port}/get_pings_received\")\n        success = 200 <= resp.status_code < 300\n        if not success:\n            log_net(warn, f\"failed to get number of pings received\", \"mock_service\", self.port)\n        else:\n            log_net(info, f\"got number of pings received\", \"mock_service\", self.port)\n        return int(resp.text)\n\n\n@dataclass\nclass PingingJob:\n    mail1: str\n    mail2: str\n    period: int\n    url: str | MockServiceHandle\n    alerting_window: int\n    response_time: int\n\n\ndef handle_child_death(signum, frame):\n    error(\"Child death detected.\\n Terminating.\")\n    exit(1)\n\n\nclass AlertingServiceHandle:\n    serial_no = 0\n    def __init__(self, log_dir: str):\n        log_net(info, \"Creating...\", \"alerting service\", 8080)\n        self.port = 8080\n        self.log_filename = f\"{log_dir}/alert-{self.port}-{AlertingServiceHandle.serial_no}.log\"\n        AlertingServiceHandle.serial_no += 1\n        log_file = open(self.log_filename, \"a\")\n        self.child_process = None\n        self.child_process = subprocess.Popen([\"python\", \"../../server/main.py\", \">\", \"server.log\"], stdout=log_file, stderr=log_file)\n        log_file.close()\n\n\n    def add_pinging_job(self, job_data: PingingJob) -> int:\n        payload = {\n            \"url\": job_data.url if isinstance(job_data.url, str) else f\"http://localhost:{job_data.url.port}/pinging_endpoint\",\n            \"alerting_window\": job_data.alerting_window,\n            \"period\": job_data.period,\n            \"primary_email\": job_data.mail1,\n            \"secondary_email\": job_data.mail2,\n            \"response_time\": job_data.response_time\n        }\n        resp = requests.post(f\"http://localhost:{self.port}/add_service\", json=payload)\n        try:\n            job_id = resp.json()[\"job_id\"]\n            log_net(info, f\"added ping job {job_id}\", self.__class__.__name__, self.port)\n            return job_id\n        except (requests.exceptions.JSONDecodeError, KeyError) as e:\n            log_net(warn, f\"failed to add ping job. Exception: {type(e).__name__}, reason: {e}, server_resp: {resp} {resp.json()}\", self.__class__.__name__, self.port)\n            return -1\n\n\n    def remove_pinging_job(\n            self, job_id: int\n    ) -> bool:\n        resp = requests.delete(f\"http://localhost:{self.port}/del_job?job_id={job_id}\")\n        success = 200 <= resp.status_code < 300\n        if not success:\n            log_net(warn, f\"failed to delete ping job with id {job_id}. response code: {resp.status_code}\", self.__class__.__name__, self.port)\n        else:\n            log_net(info, f\"deleted ping job with id {job_id}\", self.__class__.__name__, self.port)\n        return success\n\n\n    def get_pinging_jobs(\n            self, mail: str,\n    ) -> Optional[List[PingingJob]]:\n        payload = {\n            \"primary_admin_email\": mail,\n        }\n        resp = requests.get(f\"http://localhost:{self.port}/alerting_jobs\", payload)\n        if resp.status_code != 200:\n            log_net(warn, f\"failed to get pinging for mail {mail}. response code: {resp.status_code}\", self.__class__.__name__, self.port)\n            return None\n        res = []\n        try:\n            for entry in resp.json():\n                res.append(PingingJob(**entry))\n        except (requests.exceptions.JSONDecodeError, Exception) as e:\n            log_net(warn, f\"failed to get pinging jobs. Exception: {type(e).__name__}, reason: {e}\", self.__class__.__name__, self.port)\n            return None\n        log_net(info, f\"get pinging jobs for mail {mail}\", self.__class__.__name__, self.port)\n        return res\n\n\n    def confirm_alert(self, link: str) -> bool:\n\n        resp = requests.get(link)\n\n        if 200 <= resp.status_code < 300:\n            log_net(info, f\"acknowledged alert: {link}\", self.__class__.__name__, self.port)\n            return True\n        else:\n            log_net(warn, f\"failed to acknowledge alert with link {link}. response code: {resp.status_code}, response: {resp.text}\", self.__class__.__name__, self.port)\n            return False\n\n\n    def close(self):\n        os.kill(self.child_process.pid, signal.SIGTERM)\n", "missing": "         host=DB_HOST,\n"}, {"prefix": "from asyncio import sleep\n\nfrom aiohttp import web\nfrom aiohttp.web import GracefulExit\nfrom sys import argv, exit, stderr, stdout\nimport signal\nfrom threading import Lock\n\n\nresponse_", "suffix": "response_modes = {'normal', 'timeout', '404'}\n\npings_ctr_lock = Lock()\npings_ctr = 0\n\n\ndef panic(where: str, reason: str) -> None:\n    stderr.write(\"error in mock server:\\n\")\n    stderr.write(f\"{where}\\n\")\n    stderr.write(\"reason:\\n\")\n    stderr.write(f\"{reason}\\n\")\n    raise GracefulExit()\n\n\nasync def get_num_of_pings(request: web.Request) -> web.Response:\n    global pings_ctr_lock\n    with pings_ctr_lock:\n        val = pings_ctr\n    return web.Response(text=str(val))\n\nasync def set_response_mode(request: web.Request) -> web.Response:\n    global response_mode\n    try:\n        mode = request.query['mode']\n        assert mode in response_modes, f\"response mode should be one of: {response_modes}\"\n    except (KeyError, AssertionError) as e:\n        panic(\"set_response_mode\", f\"{type(e).__name__}: {e}\")\n    else:\n        response_mode = mode\n        return web.Response(status=200)\n\n\nasync def pinging_endpoint(request: web.Request) -> web.Response:\n    global pings_ctr, pings_ctr_lock\n    with pings_ctr_lock:\n        pings_ctr += 1\n    match response_mode:\n        case 'normal':\n            return web.Response(status=200, text='hello world')\n        case 'timeout':\n            await sleep(100000)\n            return web.Response(status=200, text='hello world')\n        case '404':\n            return web.Response(status=404)\n        case _:\n            panic(\"pinging_endpoint\", f\"Unknown response mode '{response_mode}'\")\n\n\napp = web.Application()\napp.router.add_get('/get_pings_received', get_num_of_pings)\napp.router.add_post('/set_response_mode', set_response_mode)\napp.router.add_get('/pinging_endpoint', pinging_endpoint)\n\n\ndef handle_SIGINT(sig, frame):\n    raise GracefulExit()\n\n\nsignal.signal(signal.SIGTERM, handle_SIGINT)\n\n\nif __name__ == '__main__':\n    assert len(argv) == 3, \"usage: python mock_server.py host port\"\n    host = argv[1]\n    try:\n        port = int(argv[2])\n    except ValueError:\n        raise ValueError(\"port number must be an integer but was {}\".format(argv[2]))\n    web.run_app(app, host=host, port=port)", "missing": "mode = 'normal'\n"}, {"prefix": "from asyncio import sleep\n\nfrom aiohttp import web\nfrom aiohttp.web import GracefulExit\nfrom sys import argv, exit, stderr, stdout\nimport signal\nfrom threading import Lock\n\n\nresponse_mode = 'normal'\nresponse_modes = {'normal', 'timeout', '404'}\n\npings_ctr_lock = Lock()\npings_ctr = 0\n\n\ndef panic(where: str, reason: str) -> None:\n    stderr.write(\"error in mock server:\\n\")\n    stderr.write(f\"{where}\\n\")\n    stderr.write(\"reason:\\n\")\n    stderr.write(f\"{reason}\\n\")\n    raise GracefulExit()\n\n\nasync def get_num_of_pings(request: web.Request) -> web.Response:\n    global pings_ctr_lock\n    with pings_ctr_lock:\n        val = pings_ctr\n    return web.Response(text=str(val))\n\nasync def set_response_mode(request: web.Request) -> web.Response:\n    global response_mode\n    try:\n        mode = request.query['mode']\n        assert mode in response_modes, f\"response mode should be one of: {response_modes}\"\n    except (KeyError, AssertionError) as e:\n        panic(\"set_response_mode\", f\"{type(e).__name__}: {e}\")\n    else:\n        response_mode = mode\n        return web.Response(status=200)\n\n\nasync def pinging_endpoint(request: web.Request) -> web.Response:\n    global pings_ctr, pings_ctr_lock\n    with pings_ctr_lock:\n        pings_ctr += 1\n    match response_mode:\n        case 'normal':\n            return web.Response(status=200, text='hello world')\n        case 'timeout':\n            await sleep(100000)\n            return web.Response(status=200, text='hello world')\n        case '404':\n            return web.Response(status=404)\n        case _:\n            panic(\"pinging_endpoint\", f\"Unknown response mode '{response_mode}'\")\n\n\napp = web.Application()\napp.router.add_get('/get_pings_received', get_num_of_pings)\napp.router.add_post('/set_response_mode', set_response_mode)\napp.router.add_get('/pinging_endpoint', pinging_endpoint)\n\n\ndef handle_SIGINT(sig, frame):\n    r", "suffix": "\n\nsignal.signal(signal.SIGTERM, handle_SIGINT)\n\n\nif __name__ == '__main__':\n    assert len(argv) == 3, \"usage: python mock_server.py host port\"\n    host = argv[1]\n    try:\n        port = int(argv[2])\n    except ValueError:\n        raise ValueError(\"port number must be an integer but was {}\".format(argv[2]))\n    web.run_app(app, host=host, port=port)", "missing": "aise GracefulExit()\n"}, {"prefix": "import sys\nfrom pathlib import Path\n\nserver_dir = Path(__file__).parent.parent.parent / \"server\"\nsys.path.append(str(server_dir))\n\nimport pytest\nimport asyncio\nfrom unittest.mock import AsyncMock, patch\nfrom aiohttp import web\nimport main\nfrom common import JobData\n\n\nexample_payload = {\n    \"url\": \"http://example.com\",\n    \"primary_email\": \"primary@example.com\",\n    \"secondary_email\": \"secondary@example.com\",\n    \"period\": 10,\n    \"alerting_window\": 5,\n    \"response_time\": 2\n}\n\n\ndef setup_app():\n    app = web.Application()\n    app.router.add_post(\"/add_service\", main.add_service)\n    app.router.add_get(\"/receive_alert\", main.receive_alert)\n    app.router.add_get(\"/get_alerting_jobs\", main.get_alerting_jobs)\n    app.router.add_delete('/del_job', main.del_job)\n    return app\n\n\n@pytest.mark.asyncio\nasync def test_add_service_success(aiohttp_client):\n    with patch(\"main.db_access.save_job\", return_value=123), \\\n         patch(\"main.new_job\", new_callable=AsyncMock):\n\n    ", "suffix": "\n        payload = example_payload.copy()\n\n        resp = await test_client.post(\"/add_service\", json=payload)\n        assert resp.status == 200\n        data = await resp.json()\n        assert data == {\"success\": True, \"job_id\": 123}\n\n\n@pytest.mark.asyncio\nasync def test_add_service_missing_keys(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    payload = {\"url\": \"http://example.com\"}\n    resp = await test_client.post(\"/add_service\", json=payload)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_add_service_invalid_types(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    payload = example_payload.copy()\n    payload[\"period\"] = \"ten\"\n\n    resp = await test_client.post(\"/add_service\", json=payload)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_add_service_non_positive_values(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    payload = example_payload.copy()\n    payload[\"response_time\"] = -1\n\n    resp = await test_client.post(\"/add_service\", json=payload)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_receive_alert_success(aiohttp_client):\n    with patch(\"main.db_access.update_notification_response_status\", return_value=True):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {\"notification_id\": \"42\"}\n\n        resp = await test_client.get(\"/receive_alert\", params=params)\n        assert resp.status == 200\n        data = await resp.json()\n        assert data == {\"success\": True}\n\n\n@pytest.mark.asyncio\nasync def test_receive_alert_missing_keys(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    params = {}\n\n    resp = await test_client.get(\"/receive_alert\", params=params)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_receive_alert_invalid_notification_id(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    params = {\"notification_id\": \"not_a_number\"}\n\n    resp = await test_client.get(\"/receive_alert\", params=params)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_get_alerting_jobs_success(aiohttp_client):\n    jobData = JobData(\"primary@example.com\", \"secondary@example.com\", \"https://example.com\", 12, 54, 42, 1, False)\n    with patch(\"main.db_access.get_jobs\", return_value=[jobData,]):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {'primary_email': 'primary@example.com'}\n\n        resp = await test_client.get(\"/get_alerting_jobs\", params=params)\n        assert resp.status == 200\n        data = await resp.json()\n        assert data == {\"jobs\": [jobData._asdict(),]}\n\n\n@pytest.mark.asyncio\nasync def test_get_alerting_jobs_missing_primary_email(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    resp = await test_client.get(\"/get_alerting_jobs\")\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_get_alerting_jobs_db_error(aiohttp_client):\n    with patch(\"main.db_access.get_jobs\", side_effect=Exception(\"Database error\")):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {'primary_email': 'test@example.com'}\n        resp = await test_client.get(\"/get_alerting_jobs\", params=params)\n        assert resp.status == 500\n\n\n@pytest.mark.asyncio\nasync def test_del_job_success(aiohttp_client):\n    with patch(\"main.db_access.set_job_inactive\", return_value=None):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {'job_id': '1'}\n        resp = await test_client.delete(\"/del_job\", params=params)\n        assert resp.status == 200\n        data = await resp.json()\n        assert data == {'success': True}\n\n\n@pytest.mark.asyncio\nasync def test_del_job_missing_job_id(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    resp = await test_client.delete(\"/del_job\")\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_del_job_db_error(aiohttp_client):\n    with patch(\"main.db_access.set_job_inactive\", side_effect=Exception(\"Database error\")):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {'job_id': '1'}\n        resp = await test_client.delete(\"/del_job\", params=params)\n        assert resp.status == 500\n", "missing": "    test_client = await aiohttp_client(setup_app())\n"}, {"prefix": "import sys\nfrom pathlib import Path\n\nserver_dir = Path(__file__).parent.parent.parent / \"server\"\nsys.path.append(str(server_dir))\n\nimport", "suffix": "import asyncio\nfrom unittest.mock import AsyncMock, patch\nfrom aiohttp import web\nimport main\nfrom common import JobData\n\n\nexample_payload = {\n    \"url\": \"http://example.com\",\n    \"primary_email\": \"primary@example.com\",\n    \"secondary_email\": \"secondary@example.com\",\n    \"period\": 10,\n    \"alerting_window\": 5,\n    \"response_time\": 2\n}\n\n\ndef setup_app():\n    app = web.Application()\n    app.router.add_post(\"/add_service\", main.add_service)\n    app.router.add_get(\"/receive_alert\", main.receive_alert)\n    app.router.add_get(\"/get_alerting_jobs\", main.get_alerting_jobs)\n    app.router.add_delete('/del_job', main.del_job)\n    return app\n\n\n@pytest.mark.asyncio\nasync def test_add_service_success(aiohttp_client):\n    with patch(\"main.db_access.save_job\", return_value=123), \\\n         patch(\"main.new_job\", new_callable=AsyncMock):\n\n        test_client = await aiohttp_client(setup_app())\n\n        payload = example_payload.copy()\n\n        resp = await test_client.post(\"/add_service\", json=payload)\n        assert resp.status == 200\n        data = await resp.json()\n        assert data == {\"success\": True, \"job_id\": 123}\n\n\n@pytest.mark.asyncio\nasync def test_add_service_missing_keys(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    payload = {\"url\": \"http://example.com\"}\n    resp = await test_client.post(\"/add_service\", json=payload)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_add_service_invalid_types(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    payload = example_payload.copy()\n    payload[\"period\"] = \"ten\"\n\n    resp = await test_client.post(\"/add_service\", json=payload)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_add_service_non_positive_values(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    payload = example_payload.copy()\n    payload[\"response_time\"] = -1\n\n    resp = await test_client.post(\"/add_service\", json=payload)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_receive_alert_success(aiohttp_client):\n    with patch(\"main.db_access.update_notification_response_status\", return_value=True):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {\"notification_id\": \"42\"}\n\n        resp = await test_client.get(\"/receive_alert\", params=params)\n        assert resp.status == 200\n        data = await resp.json()\n        assert data == {\"success\": True}\n\n\n@pytest.mark.asyncio\nasync def test_receive_alert_missing_keys(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    params = {}\n\n    resp = await test_client.get(\"/receive_alert\", params=params)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_receive_alert_invalid_notification_id(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    params = {\"notification_id\": \"not_a_number\"}\n\n    resp = await test_client.get(\"/receive_alert\", params=params)\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_get_alerting_jobs_success(aiohttp_client):\n    jobData = JobData(\"primary@example.com\", \"secondary@example.com\", \"https://example.com\", 12, 54, 42, 1, False)\n    with patch(\"main.db_access.get_jobs\", return_value=[jobData,]):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {'primary_email': 'primary@example.com'}\n\n        resp = await test_client.get(\"/get_alerting_jobs\", params=params)\n        assert resp.status == 200\n        data = await resp.json()\n        assert data == {\"jobs\": [jobData._asdict(),]}\n\n\n@pytest.mark.asyncio\nasync def test_get_alerting_jobs_missing_primary_email(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    resp = await test_client.get(\"/get_alerting_jobs\")\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_get_alerting_jobs_db_error(aiohttp_client):\n    with patch(\"main.db_access.get_jobs\", side_effect=Exception(\"Database error\")):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {'primary_email': 'test@example.com'}\n        resp = await test_client.get(\"/get_alerting_jobs\", params=params)\n        assert resp.status == 500\n\n\n@pytest.mark.asyncio\nasync def test_del_job_success(aiohttp_client):\n    with patch(\"main.db_access.set_job_inactive\", return_value=None):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {'job_id': '1'}\n        resp = await test_client.delete(\"/del_job\", params=params)\n        assert resp.status == 200\n        data = await resp.json()\n        assert data == {'success': True}\n\n\n@pytest.mark.asyncio\nasync def test_del_job_missing_job_id(aiohttp_client):\n    test_client = await aiohttp_client(setup_app())\n\n    resp = await test_client.delete(\"/del_job\")\n    assert resp.status == 400\n\n\n@pytest.mark.asyncio\nasync def test_del_job_db_error(aiohttp_client):\n    with patch(\"main.db_access.set_job_inactive\", side_effect=Exception(\"Database error\")):\n        test_client = await aiohttp_client(setup_app())\n\n        params = {'job_id': '1'}\n        resp = await test_client.delete(\"/del_job\", params=params)\n        assert resp.status == 500\n", "missing": " pytest\n"}, {"prefix": "import sys\nfrom pathlib import Path\n\nserver_dir = Path(__file__).parent.parent.parent / \"server\"\nsys.path.append(str(server_dir))\n\nimport pytest\nimport pytest_postgresql\nfrom datetime import datetime\nimport db_access\nfrom common import JobData, NotificationData\n\n\ndef setup_db(conn):\n    setup_file = server_dir / \"db_migrations/V2__setup.sql\"\n    with open(setup_file, 'r') as file:\n        sql_script = file.read()\n    \n    cursor = conn.cursor()\n    cursor.execute(sql_script)\n    conn.commit()\n\n\nEXAMPLE_JOBS = [\n    JobData(1, \"mail1@example.com\", \"mail2@example.com\", \"http://example.com\", 10, 11, 12, True),\n    JobData(2, \"mail3@example.com\", \"mail2@example.com\", \"http://ugabuga.com\", 100, 100, 200, True),\n    JobData(3, \"mail4@example.com\", \"mail5@example.com\", \"http://service.com\", 1000, 10, 100, False)\n]\n\nEXAMPLE_JOBS_PODS = [0, 1, 1]\n\n\ndef insert_example_jobs(conn):\n    cursor = conn.cursor()\n    for job, pod in zip(EXAMPLE_JOBS, EXAMPLE_JOBS_PODS):\n        cursor.execute(\"INSERT INTO jobs VALUES (DEFAULT, %s, %s, %s, %s, %s, %s, %s, %s)\",\n                       (job.mail1, job.mail2, job.url, job.period, job.window, job.response_time, pod, job.is_active))\n    conn.commit()\n\n\ndef test_db_access_set_job_inactive(postgresql):\n    setup_db(postgresql)\n\n    cursor = postgresql.cursor()\n    cursor.execute(\"INSERT INTO jobs VALUES (DEFAULT, 'mail1@example.com', 'mail2@example.com', 'http://example.com', 10, 20, 30, 0, true);\")\n    cursor.execute(\"INSERT INTO jobs VALUES (DEFAULT, 'mail3@example.com', 'mail4@example.com', 'http://another.com', 11, 21, 31, 1, true);\")\n    postgresql.commit()\n\n    cursor.execute(\"SELECT job_id FROM jobs WHERE is_active;\")\n    jobs_before = cursor.fetchall()\n    assert len(jobs_before) == 2\n\n    id_to_deactivate = jobs_before[0][0]\n\n    db_access.set_job_inactive(id_to_deactivate, postgresql)\n\n    cursor.execute(\"SELECT job_id FROM jobs WHERE is_active;\")\n    jobs_after = cursor.fetchall()\n    assert len(jobs_after) == 1\n    assert jobs_after[0][0] != id_to_deactivate\n\n    cursor.close()\n\n\ndef test_db_access_save_job(postgresql):\n    setup_db(postgresql)\n\n    job = JobData(\n        job_id=-1,\n        mail1=\"mail1@example.com\",\n        mail2=\"mail2@example.com\",\n        url=\"http://example.com\",\n        period=30,\n        window=10,\n        response_time=200,\n        is_active=True\n    )\n    \n    set_idx = 1\n\n    job_id = db_access.save_job(job, postgresql, set_idx)\n    \n    cursor = postgresql.cursor()\n    cursor.execute(\"SELECT job_id FROM jobs;\")\n    result = cursor.fetchone()\n    \n    assert result is not None\n    as", "suffix": "\n\ndef test_db_access_get_jobs(postgresql):\n    setup_db(postgresql)\n\n    primary_email = \"mail1@example.com\"\n\n    cursor = postgresql.cursor()\n    cursor.execute(f\"INSERT INTO jobs VALUES (DEFAULT, '{primary_email}', 'mail2@example.com', 'http://example.com', 10, 20, 30, 0, true);\")\n    cursor.execute(f\"INSERT INTO jobs VALUES (DEFAULT, '{primary_email}', 'mail3@example.com', 'http://another.com', 11, 21, 31, 1, false);\")\n    cursor.execute(\"INSERT INTO jobs VALUES (DEFAULT, 'mail4@example.com', 'mail5@example.com', 'http://yetanother.com', 12, 22, 32, 2, true);\")\n    postgresql.commit()\n\n    jobs = db_access.get_jobs(primary_email, postgresql)\n\n    assert len(jobs) == 2\n\n    job1, job2 = jobs\n    assert job1.mail1 == primary_email\n    assert job2.mail1 == primary_email\n    assert job1.mail2 == 'mail2@example.com'\n    assert job2.mail2 == 'mail3@example.com'\n    assert job1.url == \"http://example.com\"\n    assert job2.url == \"http://another.com\"\n    assert job1.period == 10\n    assert job2.period == 11\n    assert job1.window == 20\n    assert job2.window == 21\n    assert job1.response_time == 30\n    assert job2.response_time == 31\n    assert job1.is_active == True\n    assert job2.is_active == False\n\n\ndef test_db_access_save_notification(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    notification = NotificationData(\n        notification_id=-1,\n        time_sent=datetime.now(),\n        admin_responded=False,\n        notification_num=1,\n        job_id=1\n    )\n\n    notification_id = db_access.save_notification(notification, postgresql)\n    \n    cursor = postgresql.cursor()\n    cursor.execute(\"SELECT notification_id FROM notifications;\")\n    result = cursor.fetchone()\n    \n    assert result is not None\n    assert result[0] == notification_id\n\n\ndef test_db_access_get_notification_by_id(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    cursor = postgresql.cursor()\n    cursor.execute(\"INSERT INTO notifications VALUES (0, CURRENT_TIMESTAMP, FALSE, 1, 1);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (1, CURRENT_TIMESTAMP, TRUE, 2, 3);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (2, CURRENT_TIMESTAMP, TRUE, 1, 3);\")\n    postgresql.commit()\n\n    notification = db_access.get_notification_by_id(1, postgresql)\n\n    assert notification.admin_responded == True\n    assert notification.notification_num == 2\n\n\ndef test_db_access_update_notification_response_status(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    notification_id = 0\n\n    cursor = postgresql.cursor()\n    cursor.execute(f\"INSERT INTO notifications VALUES ({notification_id}, CURRENT_TIMESTAMP, FALSE, 1, 3);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (1, CURRENT_TIMESTAMP, TRUE, 1, 2);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (2, CURRENT_TIMESTAMP, TRUE, 2, 1);\")\n    postgresql.commit()\n\n    db_access.update_notification_response_status(notification_id, postgresql)\n\n    cursor.execute(f\"SELECT admin_responded FROM notifications WHERE notification_id = {notification_id};\")\n    result = cursor.fetchone()\n\n    assert result is not None\n    assert result[0] == True\n\n\ndef test_db_access_get_active_job_ids(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    for pod_id in set(EXAMPLE_JOBS_PODS):\n        job_ids = db_access.get_active_job_ids(postgresql, pod_id)\n        for job_pod_id, job in zip(EXAMPLE_JOBS_PODS, EXAMPLE_JOBS):\n            if job_pod_id == pod_id and job.is_active:\n                assert job.job_id in job_ids\n\n\ndef test_db_access_get_jobs_for_stateful_set(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    stateful_set_index = 1\n    jobs = db_access.get_jobs_for_stateful_set(stateful_set_index, postgresql)\n\n    assert len(jobs) == 2\n\n    job1, job2 = jobs\n    assert job1.job_id == 2\n    assert job2.job_id == 3\n\n\ndef test_db_access_get_notifications_for_jobs(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    cursor = postgresql.cursor()\n    cursor.execute(\"INSERT INTO notifications VALUES (0, CURRENT_TIMESTAMP, FALSE, 1, 1);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (1, CURRENT_TIMESTAMP, TRUE, 2, 1);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (2, CURRENT_TIMESTAMP, FALSE, 1, 2);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (3, CURRENT_TIMESTAMP, TRUE, 2, 2);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (4, CURRENT_TIMESTAMP, TRUE, 2, 3);\")\n    postgresql.commit()\n\n    notifications = db_access.get_notifications_for_jobs([1, 3, 4], postgresql)\n\n    assert len(notifications) == 3\n\n    assert len(notifications[1]) == 2\n    assert len(notifications[3]) == 1\n    assert len(notifications[4]) == 0\n\n    notification_ids_1 = [n.notification_id for n in notifications[1]]\n    assert 0 in notification_ids_1\n    assert 1 in notification_ids_1\n\n    notification_ids_3 = [n.notification_id for n in notifications[3]]\n    assert 4 in notification_ids_3\n", "missing": "sert result[0] == job_id\n"}, {"prefix": "import sys\nfrom pathlib import Path\n\nserver_dir = Path(__file__).parent.parent.parent / \"server\"\nsys.path.append(str(server_dir))\n\nimport pytest\nimport pytest_postgresql\nfrom datetime import datetime\nimport db_access\nfrom common import JobData, NotificationData\n\n\ndef setup_db(conn):\n    setup_file = server_dir / \"db_migrations/V2__setup.sql\"\n    with open(setup_file, 'r') as file:\n        sql_script = file.read()\n    \n    cursor = conn.cursor()\n    cursor.execute(sql_script)\n    conn.commit()\n\n\nEXAMPLE_JOBS = [\n    JobData(1, \"mail1@example.com\", \"mail2@example.com\", \"http://example.com\", 10, 11, 12, True),\n    JobData(2, \"mail3@example.com\", \"mail2@example.com\", \"http://ugabuga.com\", 100, 100, 200, True),\n    JobData(3, \"mail4@example.com\", \"mail5@example.com\", \"http://service.com\", 1000, 10, 100, False)\n]\n\nEXAMPLE_JOBS_PODS = [0, 1, 1]\n\n\ndef insert_example_jobs(conn):\n    cursor = conn.cursor()\n    for job, pod in zip(EXAMPLE_JOBS, EXAMPLE_JOBS_PODS):\n        cursor.execute(\"INSERT INTO jobs VALUES (DEFAULT, %s, %s, %s, %s, %s, %s, %s, %s)\",\n                       (job.mail1, job.mail2, job.url, job.period, job.window, job.response_time, pod, job.is_active))\n    conn.commit()\n\n\ndef test_db_access_set_job_inactive(postgresql):\n    setup_db(postgresql)\n\n    cursor = postgresql.cursor()\n    cursor.execute(\"INSERT INTO jobs VALUES (DEFAULT, 'mail1@example.com', 'mail2@example.com', 'http://example.com', 10, 20, 30, 0, true);\")\n    cursor.execute(\"INSERT INTO jobs VALUES (DEFAULT, 'mail3@example.com', 'mail4@example.com', 'http://another.com', 11, 21, 31, 1, true);\")\n    postgresql.commit()\n\n    cursor.execute(\"SELECT job_id FROM jobs WHERE is_active;\")\n    jobs_before = cursor.fetchall()\n    assert len(jobs_before) == 2\n\n    id_to_deactivate = jobs_before[0][0]\n\n    db_access.set_job_inactive(id_to_deactivate, postgresql)\n\n    cursor.execute(\"SELECT job_id FROM jobs WHERE is_active;\")\n    jobs_after = cursor.fetchall()\n    assert len(jobs_after) == 1\n    assert jobs_after[0][0] != id_to_deactivate\n\n    cursor.close()\n\n\ndef test_db_access_save_job(postgresql):\n    setup_db(postgresql)\n\n    job = JobData(\n        job_id=-1,\n        mail1=\"mail1@example.com\",\n        mail2=\"mail2@example.com\",\n        url=\"http://example.com\",\n        period=30,\n        window=10,\n        response_time=200,\n        is_active=True\n    )\n    \n    set_idx = 1\n\n    job_id = db_access.save_job(job, postgresql, set_idx)\n    \n    cursor = postgresql.cursor()\n    cursor.execute(\"SELECT job_id FROM jobs;\")\n    result = cursor.fetchone()\n    \n    assert result is not None\n    assert result[0] == job_id\n\n\ndef test_db_access_get_jobs(postgresql):\n    setup_db(postgresql)\n\n    primary_email = \"mail1@example.com\"\n\n    cursor = postgresql.cursor()\n    cursor.execute(f\"INSERT INTO jobs VALUES (DEFAULT, '{primary_email}', 'mail2@example.com', 'http://example.com', 10, 20, 30, 0, true);\")\n    cursor.execute(f\"INSERT INTO jobs VALUES (DEFAULT, '{primary_email}', 'mail3@example.com', 'http://another.com', 11, 21, 31, 1, false);\")\n    cursor.execute(\"INSERT INTO jobs VALUES (DEFAULT, 'mail4@example.com', 'mail5@example.com', 'http://yetanother.com', 12, 22, 32, 2, true);\")\n    postgresql.commit()\n\n    jobs = db_access.get_jobs(primary_email, postgresql)\n\n    assert len(jobs) == 2\n\n    job1, job2 = jobs\n    assert job1.mail1 == primary_email\n    assert job2.mail1 == primary_email\n    assert job1.mail2 == 'mail2@example.com'\n    assert job2.mail2 == 'mail3@example.com'\n    assert job1.url == \"http://example.com\"\n    assert job2.url == \"http://another.com\"\n    assert job1.period == 10\n    assert job2.period == 11\n    assert job1.window == 20\n    assert job2.window == 21\n    assert job1.response_time == 30\n    assert job2.response_time == 31\n    assert job1.is_active == True\n    assert job2.is_active == False\n\n\ndef test_db_access_save_notification(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    notification = NotificationData(\n        notification_id=-1,\n        time_sent=datetime.now(),\n        admin_responded=False,\n        notification_num=1,\n        job_id=1\n    )\n\n    notification_id = db_access.save_notification(notification, postgresql)\n    \n    cursor = postgresql.cursor()\n    cursor.execute(\"SELECT notification_id FROM notifications;\")\n    result = cursor.fetchone()\n    \n    assert result is not None\n    assert result[0] == notification_id\n\n\ndef test_db_access_get_notification_by_id(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    cursor = postgresql.cursor()\n    cursor.execute(\"INSERT INTO notifications VALUES (0, CURRENT_TIMESTAMP, FALSE, 1, 1);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (1, CURRENT_TIMESTAMP, TRUE, 2, 3);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (2, CURRENT_TIMESTAMP, TRUE, 1, 3);\")\n    postgresql.commit()\n\n    notification = db_access.get_notification_by_id(1, postgresql)\n\n    assert notification.admin_responded == True\n    assert notification.notification_num == 2\n\n\ndef test_db_access_update_notification_response_status(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    notification_id = 0\n\n    cursor = postgresql.cursor()\n    cursor.execute(f\"INSERT INTO notifications VALUES ({notification_id}, CURRENT_TIMESTAMP, FALSE, 1, 3);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (1, CURRENT_TIMESTAMP, TRUE, 1, 2);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (2, CURRENT_TIMESTAMP, TRUE, 2, 1);\")\n    postgresql.commit()\n\n    db_access.update_notification_response_status(notification_id, postgresql)\n\n    cursor.execute(f\"SELECT admin_responded FROM notifications WHERE notification_id = {notification_id};\")\n    result = cursor.fetchone()\n\n    assert result is not None\n    assert result[0] == True\n\n\ndef test_db_access_get_active_job_ids(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    for pod_id in set(EXAMPLE_JOBS_PODS):\n        job_ids = db_access.get_active_job_ids(postgresql, pod_id)\n        for job_pod_id, job in zip(EXAMPLE_JOBS_PODS, EXAMPLE_JOBS):\n            if job_pod_id == pod_id and job.is_active:\n                assert job.job_id in job_ids\n\n\ndef test_db_access_get_jobs_for_stateful_set(postgresql):\n    setup_db(postgresql)\n    insert_exam", "suffix": "\n    stateful_set_index = 1\n    jobs = db_access.get_jobs_for_stateful_set(stateful_set_index, postgresql)\n\n    assert len(jobs) == 2\n\n    job1, job2 = jobs\n    assert job1.job_id == 2\n    assert job2.job_id == 3\n\n\ndef test_db_access_get_notifications_for_jobs(postgresql):\n    setup_db(postgresql)\n    insert_example_jobs(postgresql)\n\n    cursor = postgresql.cursor()\n    cursor.execute(\"INSERT INTO notifications VALUES (0, CURRENT_TIMESTAMP, FALSE, 1, 1);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (1, CURRENT_TIMESTAMP, TRUE, 2, 1);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (2, CURRENT_TIMESTAMP, FALSE, 1, 2);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (3, CURRENT_TIMESTAMP, TRUE, 2, 2);\")\n    cursor.execute(\"INSERT INTO notifications VALUES (4, CURRENT_TIMESTAMP, TRUE, 2, 3);\")\n    postgresql.commit()\n\n    notifications = db_access.get_notifications_for_jobs([1, 3, 4], postgresql)\n\n    assert len(notifications) == 3\n\n    assert len(notifications[1]) == 2\n    assert len(notifications[3]) == 1\n    assert len(notifications[4]) == 0\n\n    notification_ids_1 = [n.notification_id for n in notifications[1]]\n    assert 0 in notification_ids_1\n    assert 1 in notification_ids_1\n\n    notification_ids_3 = [n.notification_id for n in notifications[3]]\n    assert 4 in notification_ids_3\n", "missing": "ple_jobs(postgresql)\n"}, {"prefix": "import google.cloud.logging\nimport logging\n\ndef _optimizations():\n    logging.logThreads = False\n    logging.logProcesses = False\n    logging.logMultiprocessing = False\n    logging.logAsyncioTasks = False\n    \ndef setup_logging():\n    try:\n        client = google.cloud.logging.Client()\n        client.setup_logging(log_level=logging.INFO)\n        _optimizations()\n        # Function names are hardcoded in the log data \n        # for speed up (avoiding sys._getframe() calls)\n        logging._srcfile = None \n    except Exception as e:\n        logging.basicConfig(\n            level=logging.INFO, \n            format=\"%(asctime)s - %(funcName)s - %(levelname)s - %(message)s\")\n        _o", "suffix": "        logging.error(\"Error setting up Google Cloud logging: %s\", e)\n        raise e", "missing": "ptimizations()\n"}, {"prefix": "import google.cloud.logging\nimport logging\n\ndef _optimizations():\n    logging.logThreads = False\n    logging.logProcesses = False\n    logging.logMultiprocessing = False\n    logging.logAsyncioTasks = False\n    \ndef setup_logging():\n    try:\n        client = google.cloud.logging.Client()\n        client.setup_logging(log_level=logging.INFO)\n        _optimizations()\n        # Function names are hardcoded in the log data \n        # for speed ", "suffix": "        logging._srcfile = None \n    except Exception as e:\n        logging.basicConfig(\n            level=logging.INFO, \n            format=\"%(asctime)s - %(funcName)s - %(levelname)s - %(message)s\")\n        _optimizations()\n        logging.error(\"Error setting up Google Cloud logging: %s\", e)\n        raise e", "missing": "up (avoiding sys._getframe() calls)\n"}, {"prefix": "import asyncio\nimport time\nimport smtplib\nfrom queue import PriorityQueue\nfrom aiohttp import ClientSession\nfrom typing import Tuple, Optional\nfrom email.mime.text import MIMEText\nfrom datetime import datetime\nimport logging\nimport threading\n\n\nimport db_access\nfrom common import *\nfrom counters import *\n\n\nactive_jobs_cache = set()\nactive_jobs_sync_loc = threading.Lock()\ncleanup_job_initialized = False\n\nsmtp_server = os.environ.get(\"SMTP_SERVER\")\nsmtp_server = 'smtp.gmail.com' if not smtp_server else smtp_server\nsm", "suffix": "smtp_port = 587 if not smtp_port else int(smtp_port)\nsmtp_username = os.environ.get('SMTP_USERNAME')\nsmtp_password = os.environ.get('SMTP_PASSWORD')\n\n\ndef send_email(to: str, subject: str, body: str):\n    log_data = {\"function_name\": \"send_email\", \"to\": to, \"subject\": subject}\n    logging.info(\"Send email called\", extra={\"json_fields\": log_data})\n\n    msg = MIMEText(body)\n    msg['Subject'] = subject\n    msg['From'] = smtp_username\n    msg['To'] = to\n    try:\n        logging.info(\"Email sent\", extra={\"json_fields\": log_data})\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        try:\n            smtp.starttls()\n            smtp.login(smtp_username, smtp_password)\n        except Exception as e:\n            if os.getenv(\"DEBUG\") is not None:\n                logging.error(e, extra={\"json_fields\": log_data})\n            else:\n                raise e\n        smtp.sendmail(smtp_username, to, msg.as_string())\n    except Exception as e:\n        logging.error(f\"Error sending email: {e}\", extra={\"json_fields\": log_data})\n        raise e\n\n\ndef send_alert(to: str, url: str, notification_id: int):\n    log_data = {\"function_name\": \"send_alert\", \"to\": to, \"url\": url,\n                \"notification_id\": notification_id}\n    logging.info(\"Send alert called\", extra={\"json_fields\": log_data})\n\n    link = f\"http://{APP_HOST}:{APP_PORT}/receive_alert?notification_id={notification_id}\"\n    subject = \"Alert\"\n    body = f\"Alert for {url}. Click {link} to acknowledge.\"\n    send_email(to, subject, body)\n\n\nasync def pinging_task(job_data: JobData, pod_index: int):\n    global cleanup_job_initialized, active_jobs_sync_loc\n    with active_jobs_sync_loc:\n        active_jobs_cache.add(job_data.job_id)\n        if not cleanup_job_initialized:\n            logging.info(\"Starting active job updater job\")\n            cleanup_job_initialized = True\n            asyncio.create_task(active_job_updater_task(pod_index))\n\n    async def single_request():\n        is_connected = False\n        try:\n            async with ClientSession() as session:\n                PINGS_SENT_CTR.inc()\n                is_connected = True\n                HTTP_CONNS_ACTIVE_CTR.inc()\n                async with session.get(job_data.url) as response:\n                    if 200 <= response.status < 300:\n                        SUCCESSFUL_PINGS_CTR.inc()\n                    HTTP_CONNS_ACTIVE_CTR.dec()\n                    return response\n        except:\n            if is_connected:\n                HTTP_CONNS_ACTIVE_CTR.dec()\n            return None\n\n    futures: PriorityQueue[Tuple[int, asyncio.Task]] = PriorityQueue()\n    while True:\n\n        delay_start = time.time_ns()\n        with active_jobs_sync_loc:\n            if job_data.job_id not in active_jobs_cache:\n                logging.info(f\"Found that job with {job_data.job_id} is not active. finishing task.\", extra={\"json_fields\": job_data})\n                JOBS_ACTIVE_CTR.dec()\n                return\n\n        task = asyncio.create_task(single_request())\n        futures.put((time.time_ns(), task))\n\n        latest = -1\n        for (t, ftr) in futures.queue:\n            if ftr.done():\n                resp = ftr.result()\n                if resp is not None and 200 <= resp.status < 300:\n                    latest = max(latest, t)\n\n        tmp: Optional[Tuple[int, asyncio.Task]] = None\n\n        while True:\n            if futures.empty():\n                tmp = None\n                break\n            tmp = futures.get()\n            if tmp[0] <= latest:\n                continue\n            futures.put(tmp)\n            break\n\n        if tmp is not None:\n            if (time.time_ns() - tmp[0]) / 1_000_000 >= job_data.window:\n                conn = db_access.setup_connection(DB_HOST, DB_PORT)\n\n                try:\n                    notification_id = db_access.save_notification(NotificationData(-1, datetime.now(), False, 1, job_data.job_id), conn)\n                    JOBS_ACTIVE_CTR.dec()\n\n                    send_alert(job_data.mail1, job_data.url, notification_id)\n                    db_access.set_job_inactive(job_data.job_id, conn)\n\n                finally:\n                    conn.close()\n                try:\n\n                    await asyncio.sleep(job_data.response_time / 1000)\n                    conn = db_access.setup_connection(DB_HOST, DB_PORT)\n\n                    if not db_access.get_notification_by_id(notification_id, conn).admin_responded:\n                        second_notification_id = db_access.save_notification(NotificationData(-1, datetime.now(), False, 2, job_data.job_id), conn)\n                        send_alert(job_data.mail2, job_data.url, second_notification_id)\n\n                finally:\n                    conn.close()\n\n                return\n\n        delay = time.time_ns() - delay_start\n        if delay / 1_000_000 > job_data.period:\n            logging.warning(\"handling the event loop consumed more time than the pinging period! keeping pinging period cannot be guaranteed!\", extra={\"json_fields\": job_data})\n        await asyncio.sleep(max(0, job_data.period / 1000 - delay / 1_000_000))\n\n\nasync def new_job(job_data: JobData, pod_index: int):\n    JOBS_ACTIVE_CTR.inc()\n    await pinging_task(job_data, pod_index)\n\n\nasync def continue_notifications(job_data: JobData, notification_data: NotificationData):\n    log_data = {\"function_name\": \"continue_notifications\", \"job_data\": job_data._asdict()}\n    logging.info(\"Continue notifications called\", extra={\"json_fields\": log_data})\n\n    if job_data.is_active:\n        try:\n            conn = db_access.setup_connection(DB_HOST, DB_PORT)\n            db_access.set_job_inactive(job_data.job_id, conn)\n        finally:\n            conn.close()\n\n    remaining_response_time = notification_data.time_sent.timestamp() * 1000 + job_data.response_time - time.time_ns() / 1_000_000\n\n    try:\n        await asyncio.sleep(max(0, remaining_response_time / 1000))\n        conn = db_access.setup_connection(DB_HOST, DB_PORT)\n\n        notifications = db_access.get_notifications_for_jobs([job_data.job_id], conn)[job_data.job_id]\n        if not any(notification.admin_responded for notification in notifications):\n            second_notification_id = db_access.save_notification(NotificationData(-1, datetime.now(), False, 2, job_data.job_id), conn)\n            send_alert(job_data.mail2, job_data.url, second_notification_id, False)\n            await asyncio.sleep(job_data.response_time / 1000)\n        logging.info(\"Notifying complete\", extra={\"json_fields\": log_data})\n    except Exception as e:\n        logging.error(\"Error while sending a second notification: %s\", e, extra={\"json_fields\": log_data})\n    finally:\n        conn.close()\n\n\nasync def active_job_updater_task(pod_index: int):\n    global active_jobs_cache, active_jobs_sync_loc\n    \"\"\"\n    Responsible for stopping deleted jobs.\n    :param pod_index: pod index\n    :return: None\n    \"\"\"\n    while True:\n        await asyncio.sleep(1)\n        conn = db_access.setup_connection(DB_HOST, DB_PORT)\n        try:\n            active_jobs_cache_new = db_access.get_active_job_ids(conn, pod_index)\n        except:\n            active_jobs_cache_new = None\n        finally:\n            conn.close()\n        if active_jobs_cache_new is not None:\n            with active_jobs_sync_loc:\n                active_jobs_cache = active_jobs_cache_new\n", "missing": "tp_port = os.environ.get(\"SMTP_PORT\")\n"}, {"prefix": "import asyncio\nimport time\nimport smtplib\nfrom queue import PriorityQueue\nfrom aiohttp import ClientSession\nfrom typing import Tuple, Optional\nfrom email.mime.text import MIMEText\nfrom datetime import datetime\nimport logging\nimport threading\n\n\nimport db_access\nfrom common import *\nfrom counters import *\n\n\nactive_jobs_cache = set()\nactive_jobs_sync_loc = threading.Lock()\ncleanup_job_initialized = False\n\nsmtp_server = os.environ.get(\"SMTP_SERVER\")\nsmtp_server = 'smtp.gmail.com' if not smtp_server else smtp_server\nsmtp_port = os.environ.get(\"SMTP_PORT\")\nsmtp_port = 587 if not smtp_port else int(smtp_port)\nsmtp_username = os.environ.get('SMTP_USERNAME')\nsmtp_password = os.environ.get('SMTP_PASSWORD')\n\n\ndef send_email(to: str, subject: str, body: str):\n    log_data = {\"function_name\": \"send_email\", \"to\": to, \"subject\": subject}\n    logging.info(\"Send email called\", extra={\"json_fields\": log_data})\n\n    msg = MIMEText(body)\n    msg['Subject'] = subject\n    msg['From'] = smtp_username\n    msg['To'] = to\n    try:\n        logging.info(\"Email sent\", extra={\"json_fields\": log_data})\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        try:\n            smtp.starttls()\n            smtp.login(smtp_username, smtp_password)\n        except Exception as e:\n            if os.getenv(\"DEBUG\") is not None:\n                logging.error(e, extra={\"json_fields\": log_data})\n            else:\n                raise e\n        smtp.sendmail(smtp_username, to, msg.as_string())\n    except Exception as e:\n        logging.error(f\"Error sending email: {e}\", extra={\"json_fields\": log_data})\n        raise e\n\n\ndef send_alert(to: str, url: str, notification_id: int):\n    log_data = {\"function_name\": \"send_alert\", \"to\": to, \"url\": url,\n                \"notification_id\": notification_id}\n    logging.info(\"Send alert called\", extra={\"json_fields\": log_data})\n\n    link = f\"http://{APP_HOST}:{APP_PORT}/receive_alert?notification_id={notification_id}\"\n    subject = \"Alert\"\n    body = f\"Alert for {url}. Click {link} to acknowledge.\"\n    send_email(to, subject, body)\n\n\nasync def pinging_task(job_data: JobData, pod_index: int):\n    global cleanup_job_initialized, active_jobs_sync_loc\n    with active_jobs_sync_loc:\n        active_jobs_cache.add(job_data.job_id)\n        if not cleanup_job_initialized:\n            logging.info(\"Starting active job updater job\")\n            cleanup_job_initialized = True\n            asyncio.create_task(active_job_updater_task(pod_index))\n\n    async def single_request():\n        is_connected = False\n        try:\n            async with ClientSession() as session:\n                PINGS_SENT_CTR.inc()\n                is_connected = True\n                HTTP_CONNS_ACTIVE_CTR.inc()\n                async with session.get(job_data.url) as response:\n                    if 200 <= response.status < 300:\n                        SUCCESSFUL_PINGS_CTR.inc()\n                    HTTP_CONNS_ACTIVE_CTR.dec()\n                    return response\n        except:\n            if is_connected:\n                HT", "suffix": "            return None\n\n    futures: PriorityQueue[Tuple[int, asyncio.Task]] = PriorityQueue()\n    while True:\n\n        delay_start = time.time_ns()\n        with active_jobs_sync_loc:\n            if job_data.job_id not in active_jobs_cache:\n                logging.info(f\"Found that job with {job_data.job_id} is not active. finishing task.\", extra={\"json_fields\": job_data})\n                JOBS_ACTIVE_CTR.dec()\n                return\n\n        task = asyncio.create_task(single_request())\n        futures.put((time.time_ns(), task))\n\n        latest = -1\n        for (t, ftr) in futures.queue:\n            if ftr.done():\n                resp = ftr.result()\n                if resp is not None and 200 <= resp.status < 300:\n                    latest = max(latest, t)\n\n        tmp: Optional[Tuple[int, asyncio.Task]] = None\n\n        while True:\n            if futures.empty():\n                tmp = None\n                break\n            tmp = futures.get()\n            if tmp[0] <= latest:\n                continue\n            futures.put(tmp)\n            break\n\n        if tmp is not None:\n            if (time.time_ns() - tmp[0]) / 1_000_000 >= job_data.window:\n                conn = db_access.setup_connection(DB_HOST, DB_PORT)\n\n                try:\n                    notification_id = db_access.save_notification(NotificationData(-1, datetime.now(), False, 1, job_data.job_id), conn)\n                    JOBS_ACTIVE_CTR.dec()\n\n                    send_alert(job_data.mail1, job_data.url, notification_id)\n                    db_access.set_job_inactive(job_data.job_id, conn)\n\n                finally:\n                    conn.close()\n                try:\n\n                    await asyncio.sleep(job_data.response_time / 1000)\n                    conn = db_access.setup_connection(DB_HOST, DB_PORT)\n\n                    if not db_access.get_notification_by_id(notification_id, conn).admin_responded:\n                        second_notification_id = db_access.save_notification(NotificationData(-1, datetime.now(), False, 2, job_data.job_id), conn)\n                        send_alert(job_data.mail2, job_data.url, second_notification_id)\n\n                finally:\n                    conn.close()\n\n                return\n\n        delay = time.time_ns() - delay_start\n        if delay / 1_000_000 > job_data.period:\n            logging.warning(\"handling the event loop consumed more time than the pinging period! keeping pinging period cannot be guaranteed!\", extra={\"json_fields\": job_data})\n        await asyncio.sleep(max(0, job_data.period / 1000 - delay / 1_000_000))\n\n\nasync def new_job(job_data: JobData, pod_index: int):\n    JOBS_ACTIVE_CTR.inc()\n    await pinging_task(job_data, pod_index)\n\n\nasync def continue_notifications(job_data: JobData, notification_data: NotificationData):\n    log_data = {\"function_name\": \"continue_notifications\", \"job_data\": job_data._asdict()}\n    logging.info(\"Continue notifications called\", extra={\"json_fields\": log_data})\n\n    if job_data.is_active:\n        try:\n            conn = db_access.setup_connection(DB_HOST, DB_PORT)\n            db_access.set_job_inactive(job_data.job_id, conn)\n        finally:\n            conn.close()\n\n    remaining_response_time = notification_data.time_sent.timestamp() * 1000 + job_data.response_time - time.time_ns() / 1_000_000\n\n    try:\n        await asyncio.sleep(max(0, remaining_response_time / 1000))\n        conn = db_access.setup_connection(DB_HOST, DB_PORT)\n\n        notifications = db_access.get_notifications_for_jobs([job_data.job_id], conn)[job_data.job_id]\n        if not any(notification.admin_responded for notification in notifications):\n            second_notification_id = db_access.save_notification(NotificationData(-1, datetime.now(), False, 2, job_data.job_id), conn)\n            send_alert(job_data.mail2, job_data.url, second_notification_id, False)\n            await asyncio.sleep(job_data.response_time / 1000)\n        logging.info(\"Notifying complete\", extra={\"json_fields\": log_data})\n    except Exception as e:\n        logging.error(\"Error while sending a second notification: %s\", e, extra={\"json_fields\": log_data})\n    finally:\n        conn.close()\n\n\nasync def active_job_updater_task(pod_index: int):\n    global active_jobs_cache, active_jobs_sync_loc\n    \"\"\"\n    Responsible for stopping deleted jobs.\n    :param pod_index: pod index\n    :return: None\n    \"\"\"\n    while True:\n        await asyncio.sleep(1)\n        conn = db_access.setup_connection(DB_HOST, DB_PORT)\n        try:\n            active_jobs_cache_new = db_access.get_active_job_ids(conn, pod_index)\n        except:\n            active_jobs_cache_new = None\n        finally:\n            conn.close()\n        if active_jobs_cache_new is not None:\n            with active_jobs_sync_loc:\n                active_jobs_cache = active_jobs_cache_new\n", "missing": "TP_CONNS_ACTIVE_CTR.dec()\n"}, {"prefix": "from collections import namedtuple\nimport os\n\n\njob_id_t = int\nnotification_id_t = int\nJobData = namedtuple(\"JobData\", [\"job_id\", \"mail1\", \"mail2\", \"url\", \"period\", \"window\", \"response_time\", \"is_active\"])\nNotificationData = namedtuple(\"NotificationData\", [\"notification_id\", \"time_sent\", \"admin_responded\", \"notification_num\", \"job_id\"])\n\n\nDB_HOST = os.environ.get(\"DB_HOST\")\nDB_PORT = 5432\n\nAPP_HOST = os.environ.get(\"APP_HOST\")\nAPP_POR", "suffix": "\nERR_MSG_CREATE_POSITIVE_INT = \"fields 'period', 'alerting_window' and 'response_time' should be positive integers\"\n", "missing": "T = 8080\n"}, {"prefix": "from collections import namedtuple\nimport os\n\n\njob_id_t = int\nnotification_id_t = int\nJobData = namedtuple(\"JobData\", [\"job_id\", \"mail1\", \"mail2\", \"url\", \"period\", \"window\", \"response_time\", \"is_active\"])\nNotificationData = namedtuple(\"NotificationData\", [\"notification_id\", \"time_sent\", \"admin_responded\", \"notification_num\", \"job_id\"])\n\n\nDB_HOST = os.environ.get(\"DB_HOST\")\nDB_", "suffix": "\nAPP_HOST = os.environ.get(\"APP_HOST\")\nAPP_PORT = 8080\n\nERR_MSG_CREATE_POSITIVE_INT = \"fields 'period', 'alerting_window' and 'response_time' should be positive integers\"\n", "missing": "PORT = 5432\n"}, {"prefix": "import os\n\nfrom aiohttp import web\nfrom aiohttp.web_runner import GracefulExit\nfrom aiohttp_sw", "suffix": "import asyncio\nfrom prometheus_client import Counter, generate_latest, CONTENT_TYPE_LATEST\nfrom counters import *\nimport logging\nimport signal\nimport os, sys\nimport logging\n\nfrom common import *\nimport db_access\nfrom coroutines import new_job, continue_notifications\nfrom logging_setup import setup_logging\n\nSTATEFUL_SET_INDEX = int(os.getenv('STATEFUL_SET_INDEX'))\n\ndb_conn = db_access.setup_connection(DB_HOST, DB_PORT)\n\n\nasync def metrics_handler(request):\n    \"\"\"Expose Prometheus metrics.\"\"\"\n    return web.Response(body=generate_latest(), content_type=CONTENT_TYPE_LATEST.rsplit(';', 1)[0])\n\n\nasync def health_handler(request):\n    \"\"\"Health check endpoint.\"\"\"\n    return web.Response(text=\"OK\", status=200)\n\n\nasync def add_service(request: web.Request):\n    \"\"\"\n    ---\n    description: Adds a service to monitor.\n    tags:\n      - Service Monitoring\n    consumes:\n      - application/json\n    produces:\n      - application/json\n    parameters:\n      - in: body\n        name: body\n        required: true\n        schema:\n          type: object\n          properties:\n            url:\n              type: string\n              description: URL of the service.\n              example: \"https://www.google.com/\"\n            primary_email:\n              type: string\n              description: Email of the primary administrator.\n              example: \"primary@example.com\"\n            secondary_email:\n              type: string\n              description: Email of the secondary administrator.\n              example: \"secondary@gmail.com\"\n            period:\n              type: integer\n              description: Service check period in ms.\n              example: 10000\n            alerting_window:\n              type: integer\n              description: Alerting window in ms.\n              example: 10000\n            response_time:\n              type: integer\n              description: Response time in ms.\n              example: 10000\n    responses:\n      \"200\":\n        description: Successful response\n        schema:\n          type: object\n          properties:\n            success:\n              type: boolean\n              example: True\n            job_id:\n              type: integer\n              example: 0\n    \"\"\"\n    log_data = {\"function_name\" : \"add_service\"}\n    logging.info(\"Add service request received\", extra={\"json_fields\" : log_data})\n\n    json = await request.json()\n    try:\n        url = json['url']\n        mail1 = json['primary_email']\n        mail2 = json['secondary_email']\n        period = json['period']\n        alerting_window = json['alerting_window']\n        response_time = json['response_time']\n    except KeyError as e:\n        logging.error(\"Missing key in request: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n    if not isinstance(period, int) or not isinstance(alerting_window, int) or not isinstance(response_time, int):\n        logging.error(\"Invalid data type for period, alerting_window or response_time\",\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': ERR_MSG_CREATE_POSITIVE_INT}, status=400)\n    if period <= 0 or alerting_window <= 0 or response_time <= 0:\n        logging.error(\"Non-positive value for period, alerting_window or response_time\",\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': ERR_MSG_CREATE_POSITIVE_INT}, status=400)\n\n    job_data = JobData(-1, mail1, mail2, url ,period, alerting_window, response_time, True)\n    try:\n        job_id = db_access.save_job(job_data, db_conn, STATEFUL_SET_INDEX)\n    except Exception as e:\n        logging.error(\"Error saving job to database: %s\", e,\n                      extra={\"json_fields\" : {**log_data, \"job_data\" : job_data._asdict()}})\n        return web.json_response({'error': str(e)}, status=501)\n    job_data = JobData(job_id, mail1, mail2, url, period, alerting_window, response_time, True)\n    asyncio.create_task(new_job(job_data, STATEFUL_SET_INDEX))\n\n    logging.info(\"Service added\",\n                 extra={\"json_fields\" : {**log_data, \"job_data\" : job_data._asdict()}})\n    return web.json_response({'success': True, 'job_id': job_id}, status=200)\n\n\nasync def receive_alert(request: web.Request):\n    \"\"\"\n    ---\n    description: Confirms that an alert was received.\n    tags:\n      - Service Monitoring\n    produces:\n      - application/json\n    parameters:\n      - in: query\n        name: notification_id\n        required: true\n        type: integer\n        description: ID of the received notification.\n        example: 0\n    responses:\n      \"200\":\n        description: Successful response\n        schema:\n          type: object\n          properties:\n            success:\n              type: boolean\n              example: True\n    \"\"\"\n    log_data = {\"function_name\" : \"receive_alert\"}\n    logging.info(\"Receive alert request received\", extra={\"json_fields\" : log_data})\n\n    try:\n        notification_id = int(request.query['notification_id'])\n    except KeyError as e:\n        logging.error(\"Missing key in request: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n    except ValueError as e:\n        logging.error(\"Invalid value for notification_id: %s\", e,\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n\n    log_data.update({\"notification_id\": notification_id})\n    try:\n        if not db_access.update_notification_response_status(notification_id, db_conn):\n            logging.info(f\"Tried to update notification with {notification_id} ID, no changes to db were made\", extra={\"json_fields\" : log_data})\n            return web.json_response({'error': \"Alert already acknowledged or does not exist\"}, status=400)\n    except Exception as e:\n        logging.error(\"Error updating alert response status: %s\", e,\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=500)\n\n    logging.info(\"Alert received\", extra={\"json_fields\" : log_data})\n    return web.json_response({'success': True}, status=200)\n\n\nasync def get_alerting_jobs(request: web.Request):\n    \"\"\"\n    ---\n    description: Returns data of alerting jobs with a specified primary administrator's email.\n    tags:\n      - Service Monitoring\n    produces:\n      - application/json\n    parameters:\n      - in: query\n        name: primary_email\n        required: true\n        type: string\n        description: Email of the primary administrator.\n        example: \"primary@example.com\"\n    responses:\n      \"200\":\n        description: Successful response\n        schema:\n          type: object\n          properties:\n            jobs:\n              type: array\n              example: []\n    \"\"\"\n    log_data = {\"function_name\" : \"get_alerting_jobs\"}\n    logging.info(\"Get alerting jobs request received\", extra={\"json_fields\" : log_data})\n\n    try:\n        mail1 = request.query['primary_email']\n    except KeyError as e:\n        logging.error(\"Missing key in request: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n\n    log_data[\"primary_email\"] = mail1\n    try:\n        jobs = db_access.get_jobs(mail1, db_conn)\n    except Exception as e:\n        logging.error(\"Error getting jobs from database: %s\", e,\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=500)\n    resp = {\"jobs\": []}\n    for job in jobs:\n        resp[\"jobs\"].append(job._asdict())\n    logging.info(\"Alerting jobs retrieved\", extra={\"json_fields\" : log_data})\n    return web.json_response(resp, status=200)\n\n\nasync def del_job(request: web.Request):\n    \"\"\"\n    ---\n    description: Deletes a monitored service.\n    tags:\n      - Service Monitoring\n    produces:\n      - application/json\n    parameters:\n      - in: query\n        name: job_id\n        required: true\n        type: integer\n        description: ID of the alerting job to delete.\n        example: 0\n    responses:\n      \"200\":\n        description: Successful response\n        schema:\n          type: object\n          properties:\n            success:\n              type: boolean\n              example: True\n    \"\"\"\n    log_data = {\"function_name\" : \"del_job\"}\n    logging.info(\"Delete job request received\", extra={\"json_fields\" : log_data})\n\n    try:\n        job_id = request.query['job_id']\n    except KeyError as e:\n        logging.error(\"Missing key in request: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n\n    log_data[\"job_id\"] = job_id\n    try:\n        db_access.set_job_inactive(int(job_id), db_conn)\n    except Exception as e:\n        logging.error(\"Error deleting job from database: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=500)\n    logging.info(\"Job deleted\", extra={\"json_fields\" : log_data})\n    return web.json_response({'success': True}, status=200)\n\n\nasync def hello(request: web.Request):\n    \"\"\"\n    ---\n    description: Returns a simple Hello, World! message.\n    responses:\n      200:\n        description: Successful response\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                message:\n                  type: string\n                  example: \"Hello, World!\"\n    \"\"\"\n    return web.json_response({\"message\": \"hello world\"})\n\n\nasync def recover_jobs():\n    log_data = {\"function_name\" : \"recover_jobs\"}\n    logging.info(\"Recovering jobs\", extra={\"json_fields\" : log_data})\n\n    try:\n      jobs = db_access.get_jobs_for_stateful_set(STATEFUL_SET_INDEX, db_conn)\n    except Exception as e:\n        logging.error(\"Error getting jobs from database: %s\", e, extra={\"json_fields\" : log_data})\n        return\n\n    job_dict = {job.job_id: job for job in jobs}\n\n    active_jobs_ids = [job.job_id for job in jobs if job.is_active]\n    inactive_jobs_ids = [job.job_id for job in jobs if not job.is_active]\n\n    try:\n      notifications = db_access.get_notifications_for_jobs(inactive_jobs_ids, db_conn)\n    except Exception as e:\n        logging.error(\"Error getting notifications from database: %s\", e, extra={\"json_fields\" : log_data})\n        return\n\n    pending_notifications_jobs_ids = [\n      job_id for job_id in inactive_jobs_ids\n      # if previously the pod crashed after sending an alert, but before setting the job\n      # as inactive, there can be multiple notifications for the primary admin\n      if notifications[job_id] and all(\n        notification.notification_num == 1 and not notification.admin_responded\n        for notification in notifications[job_id]\n      )\n    ]\n\n    for job_id in active_jobs_ids:\n        job = job_dict[job_id]\n        asyncio.create_task(new_job(job, STATEFUL_SET_INDEX))\n        logging.info(\"Resumed job\", extra={\"json_fields\" : {**log_data, \"job_data\" : job._asdict()}})\n    logging.info(\"Resumed all jobs\", extra={\"json_fields\" : log_data})\n\n    for job_id in pending_notifications_jobs_ids:\n        job = job_dict[job_id]\n        # get newest notification\n        notification = max(notifications[job.job_id], key=lambda x: x.time_sent)\n        asyncio.create_task(continue_notifications(job, notification))\n        logging.info(\"Resumed job notifying\", extra={\"json_fields\" : {**log_data, \"job_data\" : job._asdict()}})\n    logging.info(\"Resumed all job notifications\", extra={\"json_fields\" : log_data})\n\nasync def recover(app):\n    asyncio.create_task(recover_jobs())\n\n\napp = web.Application()\napp.on_startup.append(recover)\napp.router.add_post('/add_service', add_service)\napp.router.add_get('/receive_alert', receive_alert)\napp.router.add_get('/alerting_jobs', get_alerting_jobs)\napp.router.add_get('/metrics_handler', metrics_handler)\napp.router.add_get('/healthz', health_handler)\napp.router.add_delete('/del_job', del_job)\napp.router.add_get('/hello', hello)\nsetup_swagger(app, swagger_url=\"/api/doc\", title=\"Alerting Platform API\", description=\"API Documentation\")\n\n\ndef handle_SIGINT(signum, frame):\n    os.close(sys.stdout.fileno())\n    raise GracefulExit()\n\n\nsignal.signal(signal.SIGTERM, handle_SIGINT)\n\n\nif __name__ == '__main__':\n    try:\n        setup_logging()\n    except Exception as e:\n        logging.warning(\"Using default logging setup: %s\", e)\n\n    web.run_app(app, host='0.0.0.0', port=APP_PORT)\n", "missing": "agger import setup_swagger\n"}, {"prefix": "import os\n\nfrom aiohttp import web\nfrom aiohttp.web_runner import GracefulExit\nfrom aiohttp_swagger import setup_swagger\nimport asyncio\nfrom prometheus_client import Counter, generate_latest, CONTENT_TYPE_LATEST\nfrom counters import *\nimport logging\nimport signal\nimport os, sys\nimport logging\n\nfrom common import *\nimport db_access\nfrom coroutines import new_job, continue_notifications\nfrom logging_setup import setup_logging\n\nSTATEFUL_SET_INDEX = int(os.getenv('STATEFUL_SET_INDEX'))\n\ndb_conn = db_access.setup_connection(DB_HOST, DB_PORT)\n\n\nasync def metrics_handler(request):\n    \"\"\"Expose Prometheus metrics.\"\"\"\n    return web.Response(body=generate_latest(), content_type=CONTENT_TYPE_LATEST.rsplit(';', 1)[0])\n\n\nasync def health_handler(request):\n    \"\"\"Health check endpoint.\"\"\"\n    return web.Response(text=\"OK\", status=200)\n\n\nasync def add_service(request: web.Request):\n    \"\"\"\n    ---\n    description: Adds a service to monitor.\n    tags:\n      - Service Monitoring\n    consumes:\n      - application/json\n    produces:\n      - application/json\n    parameters:\n      - in: body\n        name: body\n        required: true\n        schema:\n          type: object\n          properties:\n            url:\n              type: string\n              description: URL of the service.\n              example: \"https://www.google.com/\"\n            primary_email:\n              type: string\n              description: Email of the primary administrator.\n              example: \"primary@example.com\"\n            secondary_email:\n              type: string\n              description: Email of the secondary administrator.\n              example: \"secondary@gmail.com\"\n            period:\n              type: integer\n              description: Service check period in ms.\n              example: 10000\n            alerting_window:\n              type: integer\n              description: Alerting window in ms.\n              example: 10000\n            response_time:\n              type: integer\n              description: Response time in ms.\n              example: 10000\n    responses:\n      \"200\":\n        description: Successful response\n        schema:\n          type: object\n          properties:\n            success:\n              type: boolean\n              example: True\n            job_id:\n              type: integer\n              example: 0\n    \"\"\"\n    log_data = {\"function_name\" : \"add_service\"}\n    logging.info(\"Add service request received\", extra={\"json_fields\" : log_data})\n\n    json = await request.json()\n    try:\n     ", "suffix": "        mail1 = json['primary_email']\n        mail2 = json['secondary_email']\n        period = json['period']\n        alerting_window = json['alerting_window']\n        response_time = json['response_time']\n    except KeyError as e:\n        logging.error(\"Missing key in request: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n    if not isinstance(period, int) or not isinstance(alerting_window, int) or not isinstance(response_time, int):\n        logging.error(\"Invalid data type for period, alerting_window or response_time\",\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': ERR_MSG_CREATE_POSITIVE_INT}, status=400)\n    if period <= 0 or alerting_window <= 0 or response_time <= 0:\n        logging.error(\"Non-positive value for period, alerting_window or response_time\",\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': ERR_MSG_CREATE_POSITIVE_INT}, status=400)\n\n    job_data = JobData(-1, mail1, mail2, url ,period, alerting_window, response_time, True)\n    try:\n        job_id = db_access.save_job(job_data, db_conn, STATEFUL_SET_INDEX)\n    except Exception as e:\n        logging.error(\"Error saving job to database: %s\", e,\n                      extra={\"json_fields\" : {**log_data, \"job_data\" : job_data._asdict()}})\n        return web.json_response({'error': str(e)}, status=501)\n    job_data = JobData(job_id, mail1, mail2, url, period, alerting_window, response_time, True)\n    asyncio.create_task(new_job(job_data, STATEFUL_SET_INDEX))\n\n    logging.info(\"Service added\",\n                 extra={\"json_fields\" : {**log_data, \"job_data\" : job_data._asdict()}})\n    return web.json_response({'success': True, 'job_id': job_id}, status=200)\n\n\nasync def receive_alert(request: web.Request):\n    \"\"\"\n    ---\n    description: Confirms that an alert was received.\n    tags:\n      - Service Monitoring\n    produces:\n      - application/json\n    parameters:\n      - in: query\n        name: notification_id\n        required: true\n        type: integer\n        description: ID of the received notification.\n        example: 0\n    responses:\n      \"200\":\n        description: Successful response\n        schema:\n          type: object\n          properties:\n            success:\n              type: boolean\n              example: True\n    \"\"\"\n    log_data = {\"function_name\" : \"receive_alert\"}\n    logging.info(\"Receive alert request received\", extra={\"json_fields\" : log_data})\n\n    try:\n        notification_id = int(request.query['notification_id'])\n    except KeyError as e:\n        logging.error(\"Missing key in request: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n    except ValueError as e:\n        logging.error(\"Invalid value for notification_id: %s\", e,\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n\n    log_data.update({\"notification_id\": notification_id})\n    try:\n        if not db_access.update_notification_response_status(notification_id, db_conn):\n            logging.info(f\"Tried to update notification with {notification_id} ID, no changes to db were made\", extra={\"json_fields\" : log_data})\n            return web.json_response({'error': \"Alert already acknowledged or does not exist\"}, status=400)\n    except Exception as e:\n        logging.error(\"Error updating alert response status: %s\", e,\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=500)\n\n    logging.info(\"Alert received\", extra={\"json_fields\" : log_data})\n    return web.json_response({'success': True}, status=200)\n\n\nasync def get_alerting_jobs(request: web.Request):\n    \"\"\"\n    ---\n    description: Returns data of alerting jobs with a specified primary administrator's email.\n    tags:\n      - Service Monitoring\n    produces:\n      - application/json\n    parameters:\n      - in: query\n        name: primary_email\n        required: true\n        type: string\n        description: Email of the primary administrator.\n        example: \"primary@example.com\"\n    responses:\n      \"200\":\n        description: Successful response\n        schema:\n          type: object\n          properties:\n            jobs:\n              type: array\n              example: []\n    \"\"\"\n    log_data = {\"function_name\" : \"get_alerting_jobs\"}\n    logging.info(\"Get alerting jobs request received\", extra={\"json_fields\" : log_data})\n\n    try:\n        mail1 = request.query['primary_email']\n    except KeyError as e:\n        logging.error(\"Missing key in request: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n\n    log_data[\"primary_email\"] = mail1\n    try:\n        jobs = db_access.get_jobs(mail1, db_conn)\n    except Exception as e:\n        logging.error(\"Error getting jobs from database: %s\", e,\n                      extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=500)\n    resp = {\"jobs\": []}\n    for job in jobs:\n        resp[\"jobs\"].append(job._asdict())\n    logging.info(\"Alerting jobs retrieved\", extra={\"json_fields\" : log_data})\n    return web.json_response(resp, status=200)\n\n\nasync def del_job(request: web.Request):\n    \"\"\"\n    ---\n    description: Deletes a monitored service.\n    tags:\n      - Service Monitoring\n    produces:\n      - application/json\n    parameters:\n      - in: query\n        name: job_id\n        required: true\n        type: integer\n        description: ID of the alerting job to delete.\n        example: 0\n    responses:\n      \"200\":\n        description: Successful response\n        schema:\n          type: object\n          properties:\n            success:\n              type: boolean\n              example: True\n    \"\"\"\n    log_data = {\"function_name\" : \"del_job\"}\n    logging.info(\"Delete job request received\", extra={\"json_fields\" : log_data})\n\n    try:\n        job_id = request.query['job_id']\n    except KeyError as e:\n        logging.error(\"Missing key in request: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=400)\n\n    log_data[\"job_id\"] = job_id\n    try:\n        db_access.set_job_inactive(int(job_id), db_conn)\n    except Exception as e:\n        logging.error(\"Error deleting job from database: %s\", e, extra={\"json_fields\" : log_data})\n        return web.json_response({'error': str(e)}, status=500)\n    logging.info(\"Job deleted\", extra={\"json_fields\" : log_data})\n    return web.json_response({'success': True}, status=200)\n\n\nasync def hello(request: web.Request):\n    \"\"\"\n    ---\n    description: Returns a simple Hello, World! message.\n    responses:\n      200:\n        description: Successful response\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                message:\n                  type: string\n                  example: \"Hello, World!\"\n    \"\"\"\n    return web.json_response({\"message\": \"hello world\"})\n\n\nasync def recover_jobs():\n    log_data = {\"function_name\" : \"recover_jobs\"}\n    logging.info(\"Recovering jobs\", extra={\"json_fields\" : log_data})\n\n    try:\n      jobs = db_access.get_jobs_for_stateful_set(STATEFUL_SET_INDEX, db_conn)\n    except Exception as e:\n        logging.error(\"Error getting jobs from database: %s\", e, extra={\"json_fields\" : log_data})\n        return\n\n    job_dict = {job.job_id: job for job in jobs}\n\n    active_jobs_ids = [job.job_id for job in jobs if job.is_active]\n    inactive_jobs_ids = [job.job_id for job in jobs if not job.is_active]\n\n    try:\n      notifications = db_access.get_notifications_for_jobs(inactive_jobs_ids, db_conn)\n    except Exception as e:\n        logging.error(\"Error getting notifications from database: %s\", e, extra={\"json_fields\" : log_data})\n        return\n\n    pending_notifications_jobs_ids = [\n      job_id for job_id in inactive_jobs_ids\n      # if previously the pod crashed after sending an alert, but before setting the job\n      # as inactive, there can be multiple notifications for the primary admin\n      if notifications[job_id] and all(\n        notification.notification_num == 1 and not notification.admin_responded\n        for notification in notifications[job_id]\n      )\n    ]\n\n    for job_id in active_jobs_ids:\n        job = job_dict[job_id]\n        asyncio.create_task(new_job(job, STATEFUL_SET_INDEX))\n        logging.info(\"Resumed job\", extra={\"json_fields\" : {**log_data, \"job_data\" : job._asdict()}})\n    logging.info(\"Resumed all jobs\", extra={\"json_fields\" : log_data})\n\n    for job_id in pending_notifications_jobs_ids:\n        job = job_dict[job_id]\n        # get newest notification\n        notification = max(notifications[job.job_id], key=lambda x: x.time_sent)\n        asyncio.create_task(continue_notifications(job, notification))\n        logging.info(\"Resumed job notifying\", extra={\"json_fields\" : {**log_data, \"job_data\" : job._asdict()}})\n    logging.info(\"Resumed all job notifications\", extra={\"json_fields\" : log_data})\n\nasync def recover(app):\n    asyncio.create_task(recover_jobs())\n\n\napp = web.Application()\napp.on_startup.append(recover)\napp.router.add_post('/add_service', add_service)\napp.router.add_get('/receive_alert', receive_alert)\napp.router.add_get('/alerting_jobs', get_alerting_jobs)\napp.router.add_get('/metrics_handler', metrics_handler)\napp.router.add_get('/healthz', health_handler)\napp.router.add_delete('/del_job', del_job)\napp.router.add_get('/hello', hello)\nsetup_swagger(app, swagger_url=\"/api/doc\", title=\"Alerting Platform API\", description=\"API Documentation\")\n\n\ndef handle_SIGINT(signum, frame):\n    os.close(sys.stdout.fileno())\n    raise GracefulExit()\n\n\nsignal.signal(signal.SIGTERM, handle_SIGINT)\n\n\nif __name__ == '__main__':\n    try:\n        setup_logging()\n    except Exception as e:\n        logging.warning(\"Using default logging setup: %s\", e)\n\n    web.run_app(app, host='0.0.0.0', port=APP_PORT)\n", "missing": "   url = json['url']\n"}, {"prefix": "import os\nfrom typing import Optional, List, Set, Dict\n\nimport psycopg2\n\nfrom common import JobData, job_id_t, NotificationData, notification_id_t\n\n\ndef setup_connection(db_host: str, db_port: int) -> Optional[psycopg2.extensions.connection]:\n    db_user = os.environ.get(\"DB_USER\")\n    db_pass = os.environ.get(\"DB_PASS\")\n    db_name = os.environ.get(\"DB_NAME\")\n\n    try:\n        conn = psycopg2.connect(\n            host=db_host,\n            port=db_port,\n            user=db_user,\n            password=db_pass,\n            database=db_name\n        )\n    except (Exception, psycopg2.DatabaseError) as error:\n        print(error)\n        return None\n\n    return conn\n\n\ndef set_job_inactive(job_id: job_id_t, conn: psycopg2.extensions.connection) -> None:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        UPDATE jobs SET is_active=false WHERE jobs.job_id = %s;\n        \"\"\",\n        (job_id,)\n    )\n    conn.commit()\n\n\ndef save_job(job: JobData, conn: psycopg2.extensions.connection, set_idx: int) -> job_id_t:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        INSERT INTO jobs VALUES (DEFAULT, %s, %s, %s, %s, %s, %s, %s, %s)\n        RETURNING job_id;\n        \"\"\",\n        (job.mail1, job.mail2, job.url, job.period, job.window, job.response_time, set_idx, job.is_active)\n    )\n    conn.commit()\n    return job_id_t(cursor.fetchone()[0])\n\n\ndef get_jobs(primary_email: str, conn: psycopg2.extensions.connection) -> List[JobData]:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        SELECT * FROM jobs WHERE mail1 = %s;\n        \"\"\",\n        (primary_email,)\n    )\n    rows = cursor.fetchall()\n\n    jobs = []\n    for row in rows:\n        jobs.append(JobData(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[8]))\n    return jobs\n\n\ndef save_notification(notification: NotificationData, conn: psycopg2.extensions.connection) -> notification_id_t:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        INSERT INTO notifications VALUES (DEFAULT, %s, %s, %s, %s)\n        RETURNING notification_id;\n        \"\"\",\n        (notification.time_sent, notification.admin_responded, notification.notification_num, notification.job_id)\n    )\n    conn.commit()\n    return notification_id_t(cursor.fetchone()[0])\n\n\ndef get_notification_by_id(notification_id: int, conn: psycopg2.extensions.connection) -> NotificationData:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        SELECT * FROM notifications WHERE notification_id = %s;\n        \"\"\",\n        (notification_id,)\n    )\n\n    return NotificationData(*cursor.fetchone())\n\n\ndef update_notification_response_status(notification_id: int, conn: psycopg2.extensions.connection) -> bool:\n    \"\"\"\n    :param notification_id:\n    :param conn:\n    :return: true if updated single row with given notification_id\n    \"\"\"\n    cursor = conn.cursor()\n\n    cursor.execute(\n        f\"\"\"\n        UPDATE notifications SET admin_responded = TRUE WHERE notification_id = %s;\n        \"\"\",\n        (notification_id,)\n    )\n\n    rowcount = cursor.rowcount\n\n    conn.commit()\n    return rowcount == 1\n\n\ndef get_active_job_ids(conn: psycopg2.extensions.connection, pod_index: int) -> Set[job_id_t]:\n    \"\"\"\n    :param conn: postgres connection\n    :param pod_index: index of pod\n    :return: list of all active jobs assigned to this pod\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT job_id FROM jobs WHERE is_active = TRUE and stateful_set_index = %s;\n        \"\"\",\n        (pod_index,)\n    )\n    conn.commit()\n\n    return {job_id_t(x[0]) for x in cursor.fetchall()}\n\n\ndef get_jobs_for_stateful_set(stateful_set_index: int, conn: psycopg2.extensions.connection) -> List[JobData]:\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT * FROM jobs WHERE stateful_set_index = %s;\n        \"\"\",\n        (stateful_set_index,)\n    )\n    rows = cursor.fetchall()\n    \n    jobs = []\n    for row in rows:\n        jobs.append(JobData(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[8]))\n    return jobs\n\n\ndef get_notifications_for_jobs(job_ids: list[job_id_t], conn: psycopg2.extensions.connection) -> Dict[job_id_t, List[NotificationData]]:\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT * FROM notifications WHERE job_id = ANY(%s);\n ", "suffix": "        (job_ids,)\n    )\n    rows = cursor.fetchall()\n\n    notifications = {job_id: [] for job_id in job_ids}\n    for row in rows:\n        notification = NotificationData(*row)\n        notifications[notification.job_id].append(notification)\n    return notifications", "missing": "       \"\"\",\n"}, {"prefix": "import os\nfrom typing import Optional, List, Set, Dict\n\nimport psycopg2\n\nfrom common import JobData, job_id_t, NotificationData, notification_id_t\n\n\ndef setup_connection(db_host: str, db_port: int) -> Optional[psycopg2.extensions.connection]:\n    db_user = os.environ.get(\"DB_USER\")\n    db_pass = os.environ.get(\"DB_PASS\")\n    db_name = os.environ.get(\"DB_NAME\")\n\n    try:\n        conn = psycopg2.connect(\n            host=db_host,\n            port=db_port,\n            user=db_user,\n            password=db_pass,\n            database=db_name\n        )\n    except (Exception, psycopg2.DatabaseError) as error:\n       ", "suffix": "        return None\n\n    return conn\n\n\ndef set_job_inactive(job_id: job_id_t, conn: psycopg2.extensions.connection) -> None:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        UPDATE jobs SET is_active=false WHERE jobs.job_id = %s;\n        \"\"\",\n        (job_id,)\n    )\n    conn.commit()\n\n\ndef save_job(job: JobData, conn: psycopg2.extensions.connection, set_idx: int) -> job_id_t:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        INSERT INTO jobs VALUES (DEFAULT, %s, %s, %s, %s, %s, %s, %s, %s)\n        RETURNING job_id;\n        \"\"\",\n        (job.mail1, job.mail2, job.url, job.period, job.window, job.response_time, set_idx, job.is_active)\n    )\n    conn.commit()\n    return job_id_t(cursor.fetchone()[0])\n\n\ndef get_jobs(primary_email: str, conn: psycopg2.extensions.connection) -> List[JobData]:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        SELECT * FROM jobs WHERE mail1 = %s;\n        \"\"\",\n        (primary_email,)\n    )\n    rows = cursor.fetchall()\n\n    jobs = []\n    for row in rows:\n        jobs.append(JobData(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[8]))\n    return jobs\n\n\ndef save_notification(notification: NotificationData, conn: psycopg2.extensions.connection) -> notification_id_t:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        INSERT INTO notifications VALUES (DEFAULT, %s, %s, %s, %s)\n        RETURNING notification_id;\n        \"\"\",\n        (notification.time_sent, notification.admin_responded, notification.notification_num, notification.job_id)\n    )\n    conn.commit()\n    return notification_id_t(cursor.fetchone()[0])\n\n\ndef get_notification_by_id(notification_id: int, conn: psycopg2.extensions.connection) -> NotificationData:\n    cursor = conn.cursor()\n    cursor.execute(\n        f\"\"\"\n        SELECT * FROM notifications WHERE notification_id = %s;\n        \"\"\",\n        (notification_id,)\n    )\n\n    return NotificationData(*cursor.fetchone())\n\n\ndef update_notification_response_status(notification_id: int, conn: psycopg2.extensions.connection) -> bool:\n    \"\"\"\n    :param notification_id:\n    :param conn:\n    :return: true if updated single row with given notification_id\n    \"\"\"\n    cursor = conn.cursor()\n\n    cursor.execute(\n        f\"\"\"\n        UPDATE notifications SET admin_responded = TRUE WHERE notification_id = %s;\n        \"\"\",\n        (notification_id,)\n    )\n\n    rowcount = cursor.rowcount\n\n    conn.commit()\n    return rowcount == 1\n\n\ndef get_active_job_ids(conn: psycopg2.extensions.connection, pod_index: int) -> Set[job_id_t]:\n    \"\"\"\n    :param conn: postgres connection\n    :param pod_index: index of pod\n    :return: list of all active jobs assigned to this pod\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT job_id FROM jobs WHERE is_active = TRUE and stateful_set_index = %s;\n        \"\"\",\n        (pod_index,)\n    )\n    conn.commit()\n\n    return {job_id_t(x[0]) for x in cursor.fetchall()}\n\n\ndef get_jobs_for_stateful_set(stateful_set_index: int, conn: psycopg2.extensions.connection) -> List[JobData]:\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT * FROM jobs WHERE stateful_set_index = %s;\n        \"\"\",\n        (stateful_set_index,)\n    )\n    rows = cursor.fetchall()\n    \n    jobs = []\n    for row in rows:\n        jobs.append(JobData(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[8]))\n    return jobs\n\n\ndef get_notifications_for_jobs(job_ids: list[job_id_t], conn: psycopg2.extensions.connection) -> Dict[job_id_t, List[NotificationData]]:\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT * FROM notifications WHERE job_id = ANY(%s);\n        \"\"\",\n        (job_ids,)\n    )\n    rows = cursor.fetchall()\n\n    notifications = {job_id: [] for job_id in job_ids}\n    for row in rows:\n        notification = NotificationData(*row)\n        notifications[notification.job_id].append(notification)\n    return notifications", "missing": " print(error)\n"}, {"prefix": "from prometheus_client import Counter, Gauge\n\n\nPINGS_SENT_CTR = Counter(", "suffix": "SUCCESSFUL_PINGS_CTR = Counter('successful_pings_total', 'Total Pings')\nHTTP_CONNS_ACTIVE_CTR = Gauge('http_conns_active_total', 'Total HTTP connections')\nJOBS_ACTIVE_CTR = Gauge('jobs_active_total', 'Total Jobs')", "missing": "'pings_sent_total', 'Total Pings')\n"}, {"prefix": "from prometheus_client import Counter, Gauge\n\n\nPINGS_SENT_CTR = Counter('pings_sent_total', 'Total Pings')\nSUCCESSFUL_PINGS_CTR = Counter('successful_pings_total', 'Total Pings')\nHTTP_CONNS_ACTIVE_CTR = Gauge('http_conns_active_total', 'Total HTTP connections')\nJOBS_ACTIVE_", "suffix": "", "missing": "CTR = Gauge('jobs_active_total', 'Total Jobs')"}]